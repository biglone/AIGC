# ç«¯åˆ°ç«¯é¡¹ç›®å®æˆ˜æ•™ç¨‹

> **æ–‡æ¡£å®šä½ï¼š** ä»0åˆ°1æ„å»ºå®Œæ•´AIGCé¡¹ç›®çš„å®æˆ˜æŒ‡å—
> **é€‚ç”¨å¯¹è±¡ï¼š** å·²æŒæ¡åŸºç¡€ç†è®ºï¼Œå¸Œæœ›æ•´åˆçŸ¥è¯†æ„å»ºå®Œæ•´ç³»ç»Ÿçš„å­¦ä¹ è€…
> **å‰ç½®çŸ¥è¯†ï¼š** å®Œæˆ01-14æ‰€æœ‰æ–‡æ¡£çš„å­¦ä¹ 

---

## ç›®å½•

1. [é¡¹ç›®æ¡ˆä¾‹1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ](#é¡¹ç›®æ¡ˆä¾‹1æ™ºèƒ½å®¢æœç³»ç»Ÿ)
2. [é¡¹ç›®æ¡ˆä¾‹2ï¼šä»£ç åŠ©æ‰‹ï¼ˆç±»Copilotï¼‰](#é¡¹ç›®æ¡ˆä¾‹2ä»£ç åŠ©æ‰‹ç±»copilot)
3. [é¡¹ç›®æ¡ˆä¾‹3ï¼šå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå¹³å°](#é¡¹ç›®æ¡ˆä¾‹3å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå¹³å°)
4. [é€šç”¨æœ€ä½³å®è·µ](#é€šç”¨æœ€ä½³å®è·µ)
5. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## é¡¹ç›®æ¡ˆä¾‹1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

> **ç›®æ ‡ï¼š** æ„å»ºä¸€ä¸ªèƒ½å›ç­”å…¬å¸äº§å“é—®é¢˜çš„æ™ºèƒ½å®¢æœï¼Œæ•´åˆRAGã€æ•°æ®å·¥ç¨‹ã€MLOpsã€ç”Ÿäº§éƒ¨ç½²å…¨æµç¨‹

### 1.1 éœ€æ±‚åˆ†æä¸æ¶æ„è®¾è®¡

#### ä¸šåŠ¡éœ€æ±‚

**åŠŸèƒ½éœ€æ±‚ï¼š**
- å›ç­”äº§å“ç›¸å…³é—®é¢˜ï¼ˆæ–‡æ¡£ã€FAQï¼‰
- æ”¯æŒå¤šè½®å¯¹è¯ï¼Œè®°å¿†ä¸Šä¸‹æ–‡
- è¯†åˆ«æ„å›¾å¹¶è·¯ç”±åˆ°äººå·¥å®¢æœï¼ˆå¤æ‚é—®é¢˜ï¼‰
- æ”¯æŒä¸­è‹±æ–‡åŒè¯­
- å“åº”æ—¶é—´<2ç§’

**éåŠŸèƒ½éœ€æ±‚ï¼š**
- å‡†ç¡®ç‡>85%ï¼ˆå›ç­”æ­£ç¡®æ€§ï¼‰
- å¯ç”¨æ€§>99.9%
- æˆæœ¬å¯æ§ï¼ˆæœˆè´¹ç”¨<$500ï¼‰
- å¯å®¡è®¡ï¼ˆè®°å½•æ‰€æœ‰å¯¹è¯ï¼‰

#### æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·ç•Œé¢    â”‚ (Web/å¾®ä¿¡/Slack)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APIç½‘å…³ï¼ˆFastAPIï¼‰          â”‚
â”‚  - è¯·æ±‚éªŒè¯                  â”‚
â”‚  - é€Ÿç‡é™åˆ¶                  â”‚
â”‚  - è´Ÿè½½å‡è¡¡                  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ™ºèƒ½å®¢æœå¼•æ“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ æ„å›¾è¯†åˆ« (åˆ†ç±»å™¨)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ RAGç³»ç»Ÿ              â”‚  â”‚
â”‚  â”‚  - æ£€ç´¢ç›¸å…³æ–‡æ¡£       â”‚  â”‚
â”‚  â”‚  - LLMç”Ÿæˆå›ç­”       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ å¯¹è¯ç®¡ç†             â”‚  â”‚
â”‚  â”‚  - ä¸Šä¸‹æ–‡è®°å¿†         â”‚  â”‚
â”‚  â”‚  - å¤šè½®å¯¹è¯           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®å±‚                      â”‚
â”‚  - å‘é‡æ•°æ®åº“(ChromaDB)      â”‚
â”‚  - å¯¹è¯å†å²(PostgreSQL)      â”‚
â”‚  - ç¼“å­˜(Redis)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### æŠ€æœ¯é€‰å‹

| ç»„ä»¶ | æŠ€æœ¯é€‰æ‹© | ç†ç”± |
|------|----------|------|
| **LLM** | GPT-4o-mini | æˆæœ¬ä½ã€é€Ÿåº¦å¿«ã€è´¨é‡å¥½ |
| **Embedding** | text-embedding-3-small | æ€§ä»·æ¯”é«˜ |
| **å‘é‡æ•°æ®åº“** | ChromaDB | è½»é‡ã€æ˜“ç”¨ã€å…è´¹ |
| **Webæ¡†æ¶** | FastAPI | å¼‚æ­¥æ”¯æŒã€æ€§èƒ½å¥½ |
| **å‰ç«¯** | Gradio | å¿«é€ŸåŸå‹ |
| **ç›‘æ§** | Prometheus + Grafana | å¼€æºã€åŠŸèƒ½å®Œå–„ |
| **éƒ¨ç½²** | Docker + K8s | æ ‡å‡†åŒ–ã€å¯æ‰©å±• |

---

### 1.2 æ•°æ®å‡†å¤‡

#### æ•°æ®æºæ”¶é›†

```python
# data_collection.py
import os
from typing import List, Dict
from pathlib import Path

class DataCollector:
    """æ”¶é›†å®¢æœæ•°æ®"""

    def __init__(self):
        self.documents = []

    def collect_product_docs(self, docs_path: str):
        """æ”¶é›†äº§å“æ–‡æ¡£"""
        for file_path in Path(docs_path).rglob("*.md"):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

                self.documents.append({
                    'source': 'product_docs',
                    'file': str(file_path),
                    'content': content,
                    'metadata': {
                        'type': 'documentation',
                        'language': 'zh' if self._is_chinese(content) else 'en'
                    }
                })

    def collect_faq(self, faq_file: str):
        """æ”¶é›†FAQæ•°æ®"""
        import json
        with open(faq_file, 'r', encoding='utf-8') as f:
            faqs = json.load(f)

        for faq in faqs:
            self.documents.append({
                'source': 'faq',
                'content': f"Q: {faq['question']}\nA: {faq['answer']}",
                'metadata': {
                    'type': 'faq',
                    'category': faq.get('category', 'general')
                }
            })

    def collect_historical_chats(self, chat_logs: List[Dict]):
        """æ”¶é›†å†å²å¯¹è¯è®°å½•"""
        for chat in chat_logs:
            if chat.get('rating', 0) >= 4:  # åªä¿ç•™é«˜è´¨é‡å¯¹è¯
                self.documents.append({
                    'source': 'chat_history',
                    'content': f"ç”¨æˆ·: {chat['user']}\nå®¢æœ: {chat['agent']}",
                    'metadata': {
                        'type': 'conversation',
                        'rating': chat['rating']
                    }
                })

    def _is_chinese(self, text: str) -> bool:
        """ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºä¸­æ–‡"""
        return sum('\u4e00' <= char <= '\u9fff' for char in text) > len(text) * 0.3

    def export_to_jsonl(self, output_path: str):
        """å¯¼å‡ºä¸ºJSONLæ ¼å¼"""
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            for doc in self.documents:
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')

# ä½¿ç”¨ç¤ºä¾‹
collector = DataCollector()
collector.collect_product_docs('docs/')
collector.collect_faq('data/faq.json')
# collector.collect_historical_chats(chat_logs)
collector.export_to_jsonl('data/raw_documents.jsonl')

print(f"æ”¶é›†äº† {len(collector.documents)} ä¸ªæ–‡æ¡£")
```

#### æ•°æ®æ¸…æ´—

```python
# data_cleaning.py
from typing import List, Dict
import re

class DataCleaner:
    """æ•°æ®æ¸…æ´—"""

    def clean_documents(self, documents: List[Dict]) -> List[Dict]:
        """æ¸…æ´—æ–‡æ¡£"""
        cleaned = []

        for doc in documents:
            # 1. ç§»é™¤è¿‡çŸ­çš„æ–‡æ¡£
            if len(doc['content']) < 50:
                continue

            # 2. ç§»é™¤é‡å¤å†…å®¹
            content = self._remove_duplicates(doc['content'])

            # 3. æ ‡å‡†åŒ–æ ¼å¼
            content = self._normalize_text(content)

            # 4. ç§»é™¤PII
            content = self._remove_pii(content)

            # 5. è´¨é‡æ£€æŸ¥
            if self._quality_check(content):
                doc['content'] = content
                cleaned.append(doc)

        return cleaned

    def _remove_duplicates(self, text: str) -> str:
        """ç§»é™¤é‡å¤æ®µè½"""
        paragraphs = text.split('\n\n')
        seen = set()
        unique = []

        for p in paragraphs:
            p_hash = hash(p.strip())
            if p_hash not in seen:
                seen.add(p_hash)
                unique.append(p)

        return '\n\n'.join(unique)

    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        # ç»Ÿä¸€ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)

        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # æ ‡å‡†åŒ–æ ‡ç‚¹
        text = text.replace('ï¼Œ', ', ').replace('ã€‚', '. ')

        return text.strip()

    def _remove_pii(self, text: str) -> str:
        """ç§»é™¤ä¸ªäººä¿¡æ¯"""
        # ç§»é™¤é‚®ç®±
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
                     '[EMAIL]', text)

        # ç§»é™¤ç”µè¯å·ç 
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', text)

        return text

    def _quality_check(self, text: str) -> bool:
        """è´¨é‡æ£€æŸ¥"""
        # æ£€æŸ¥é•¿åº¦
        if len(text) < 50 or len(text) > 10000:
            return False

        # æ£€æŸ¥ä¿¡æ¯å¯†åº¦ï¼ˆå•è¯/å­—ç¬¦æ¯”ï¼‰
        words = text.split()
        if len(words) / len(text) < 0.05:
            return False

        return True

# ä½¿ç”¨ç¤ºä¾‹
import json
with open('data/raw_documents.jsonl', 'r', encoding='utf-8') as f:
    raw_docs = [json.loads(line) for line in f]

cleaner = DataCleaner()
cleaned_docs = cleaner.clean_documents(raw_docs)

with open('data/cleaned_documents.jsonl', 'w', encoding='utf-8') as f:
    for doc in cleaned_docs:
        f.write(json.dumps(doc, ensure_ascii=False) + '\n')

print(f"æ¸…æ´—åå‰©ä½™ {len(cleaned_docs)} ä¸ªæ–‡æ¡£")
```

---

### 1.3 RAGç³»ç»Ÿå®ç°

#### æ–‡æ¡£ç´¢å¼•

```python
# indexer.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
import json

class DocumentIndexer:
    """æ–‡æ¡£ç´¢å¼•å™¨"""

    def __init__(self, persist_directory="./chroma_db"):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.persist_directory = persist_directory

        # ä¸­è‹±æ–‡åˆ†åˆ«é…ç½®
        self.text_splitters = {
            'zh': RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " "]
            ),
            'en': RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100,
                separators=["\n\n", "\n", ". ", "! ", "? ", "; ", " "]
            )
        }

    def index_documents(self, documents_path: str):
        """ç´¢å¼•æ–‡æ¡£"""
        # åŠ è½½æ–‡æ¡£
        with open(documents_path, 'r', encoding='utf-8') as f:
            documents = [json.loads(line) for line in f]

        all_chunks = []
        all_metadatas = []

        for doc in documents:
            # é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨
            lang = doc['metadata'].get('language', 'en')
            splitter = self.text_splitters.get(lang, self.text_splitters['en'])

            # åˆ†å—
            chunks = splitter.split_text(doc['content'])

            for i, chunk in enumerate(chunks):
                all_chunks.append(chunk)
                all_metadatas.append({
                    **doc['metadata'],
                    'source': doc['source'],
                    'chunk_id': i,
                    'total_chunks': len(chunks)
                })

        # åˆ›å»ºå‘é‡æ•°æ®åº“
        vectorstore = Chroma.from_texts(
            texts=all_chunks,
            embedding=self.embeddings,
            metadatas=all_metadatas,
            persist_directory=self.persist_directory
        )

        vectorstore.persist()
        print(f"ç´¢å¼•å®Œæˆ: {len(all_chunks)} ä¸ªchunk")

        return vectorstore

# ä½¿ç”¨
indexer = DocumentIndexer()
vectorstore = indexer.index_documents('data/cleaned_documents.jsonl')
```

#### æŸ¥è¯¢å¼•æ“

```python
# query_engine.py
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

class CustomerServiceQA:
    """å®¢æœé—®ç­”å¼•æ“"""

    def __init__(self, persist_directory="./chroma_db"):
        # åŠ è½½å‘é‡æ•°æ®åº“
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )

        # LLM
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0
        )

        # Promptæ¨¡æ¿
        self.qa_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å›ç­”è¦ä¸“ä¸šã€å‹å¥½ã€ç®€æ´
4. å¦‚æœé—®é¢˜å¤æ‚ï¼Œå»ºè®®ç”¨æˆ·è”ç³»äººå·¥å®¢æœ

å›ç­”ï¼š""",
            input_variables=["context", "question"]
        )

        # æ„å»ºQAé“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 3, "fetch_k": 10}
            ),
            chain_type_kwargs={"prompt": self.qa_prompt},
            return_source_documents=True
        )

    def query(self, question: str):
        """æŸ¥è¯¢"""
        result = self.qa_chain({"query": question})

        return {
            'answer': result['result'],
            'sources': [
                {
                    'content': doc.page_content,
                    'metadata': doc.metadata
                }
                for doc in result['source_documents']
            ]
        }

# ä½¿ç”¨
qa = CustomerServiceQA()
response = qa.query("å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ")
print("å›ç­”:", response['answer'])
print("æ¥æº:", len(response['sources']), "ä¸ªæ–‡æ¡£")
```

#### å¯¹è¯ç®¡ç†

```python
# conversation_manager.py
from typing import List, Dict
from collections import deque

class ConversationManager:
    """å¯¹è¯ç®¡ç†"""

    def __init__(self, max_history=5):
        self.conversations = {}  # session_id -> conversation
        self.max_history = max_history

    def add_message(self, session_id: str, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯"""
        if session_id not in self.conversations:
            self.conversations[session_id] = {
                'history': deque(maxlen=self.max_history * 2),  # user + assistant
                'metadata': {}
            }

        self.conversations[session_id]['history'].append({
            'role': role,
            'content': content
        })

    def get_context(self, session_id: str) -> str:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        if session_id not in self.conversations:
            return ""

        history = self.conversations[session_id]['history']
        context_parts = []

        for msg in history:
            if msg['role'] == 'user':
                context_parts.append(f"ç”¨æˆ·: {msg['content']}")
            else:
                context_parts.append(f"å®¢æœ: {msg['content']}")

        return "\n".join(context_parts)

    def query_with_context(self, session_id: str, question: str, qa_engine):
        """å¸¦ä¸Šä¸‹æ–‡çš„æŸ¥è¯¢"""
        # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„é—®é¢˜
        context = self.get_context(session_id)

        if context:
            enhanced_question = f"""
å¯¹è¯å†å²ï¼š
{context}

å½“å‰é—®é¢˜ï¼š{question}

è¯·åŸºäºå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜ç»™å‡ºå›ç­”ã€‚
"""
        else:
            enhanced_question = question

        # æŸ¥è¯¢
        response = qa_engine.query(enhanced_question)

        # è®°å½•å¯¹è¯
        self.add_message(session_id, 'user', question)
        self.add_message(session_id, 'assistant', response['answer'])

        return response

# ä½¿ç”¨
conv_manager = ConversationManager()
qa = CustomerServiceQA()

# å¤šè½®å¯¹è¯
session_id = "user123"

response1 = conv_manager.query_with_context(
    session_id, "ä½ ä»¬çš„äº§å“æ”¯æŒå“ªäº›å¹³å°ï¼Ÿ", qa
)
print("å›ç­”1:", response1['answer'])

response2 = conv_manager.query_with_context(
    session_id, "é‚£iOSç‰ˆæœ¬ä»€ä¹ˆæ—¶å€™ä¸Šçº¿ï¼Ÿ", qa
)
print("å›ç­”2:", response2['answer'])
```

---

### 1.4 æ„å›¾è¯†åˆ«ä¸è·¯ç”±

```python
# intent_classifier.py
from transformers import pipeline

class IntentClassifier:
    """æ„å›¾åˆ†ç±»å™¨"""

    def __init__(self):
        # ä½¿ç”¨zero-shotåˆ†ç±»å™¨
        self.classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli"
        )

        self.intent_labels = [
            "äº§å“å’¨è¯¢",      # äº§å“åŠŸèƒ½ã€ä»·æ ¼ç­‰
            "æŠ€æœ¯æ”¯æŒ",      # æŠ€æœ¯é—®é¢˜ã€bugæŠ¥å‘Š
            "è´¦æˆ·ç®¡ç†",      # ç™»å½•ã€å¯†ç ã€è´¦æˆ·é—®é¢˜
            "æŠ•è¯‰å»ºè®®",      # æŠ•è¯‰ã€å»ºè®®ã€åé¦ˆ
            "å…¶ä»–"           # é—²èŠã€æ— å…³é—®é¢˜
        ]

    def classify(self, text: str) -> Dict:
        """åˆ†ç±»æ„å›¾"""
        result = self.classifier(text, self.intent_labels)

        return {
            'intent': result['labels'][0],
            'confidence': result['scores'][0],
            'all_scores': dict(zip(result['labels'], result['scores']))
        }

    def should_transfer_to_human(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬äººå·¥"""
        intent_result = self.classify(text)

        # æŠ•è¯‰å»ºè®®ç±»é—®é¢˜è½¬äººå·¥
        if intent_result['intent'] == "æŠ•è¯‰å»ºè®®":
            return True

        # ç½®ä¿¡åº¦ä½è½¬äººå·¥
        if intent_result['confidence'] < 0.6:
            return True

        # æ£€æµ‹è´Ÿé¢æƒ…ç»ªï¼ˆç®€åŒ–ç‰ˆï¼‰
        negative_keywords = ["ä¸æ»¡æ„", "å·®åŠ²", "æŠ•è¯‰", "é€€æ¬¾", "éª—äºº"]
        if any(kw in text for kw in negative_keywords):
            return True

        return False

# é›†æˆåˆ°ä¸»æµç¨‹
class SmartCustomerService:
    """æ™ºèƒ½å®¢æœä¸»ç³»ç»Ÿ"""

    def __init__(self):
        self.intent_classifier = IntentClassifier()
        self.qa_engine = CustomerServiceQA()
        self.conv_manager = ConversationManager()

    def handle_message(self, session_id: str, message: str) -> Dict:
        """å¤„ç†æ¶ˆæ¯"""
        # 1. æ„å›¾è¯†åˆ«
        intent_result = self.intent_classifier.classify(message)

        # 2. åˆ¤æ–­æ˜¯å¦è½¬äººå·¥
        if self.intent_classifier.should_transfer_to_human(message):
            return {
                'type': 'transfer',
                'message': "æ‚¨çš„é—®é¢˜å·²è½¬æ¥äººå·¥å®¢æœï¼Œè¯·ç¨å€™...",
                'intent': intent_result
            }

        # 3. RAGé—®ç­”
        response = self.conv_manager.query_with_context(
            session_id, message, self.qa_engine
        )

        return {
            'type': 'answer',
            'message': response['answer'],
            'sources': response['sources'],
            'intent': intent_result
        }

# ä½¿ç”¨
service = SmartCustomerService()

response = service.handle_message(
    "user123",
    "ä½ ä»¬çš„äº§å“å¤ªå·®äº†ï¼Œæˆ‘è¦æŠ•è¯‰ï¼"
)

if response['type'] == 'transfer':
    print("è½¬äººå·¥:", response['message'])
else:
    print("AIå›ç­”:", response['message'])
```

---

### 1.5 APIæœåŠ¡ä¸éƒ¨ç½²

#### FastAPIæœåŠ¡

```python
# app.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import uuid

app = FastAPI(title="æ™ºèƒ½å®¢æœAPI")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æœåŠ¡å®ä¾‹
service = SmartCustomerService()

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    session_id: str
    message: str
    type: str
    intent: Optional[Dict] = None

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """èŠå¤©æ¥å£"""
    # ç”Ÿæˆæˆ–ä½¿ç”¨session_id
    session_id = request.session_id or str(uuid.uuid4())

    try:
        # å¤„ç†æ¶ˆæ¯
        response = service.handle_message(session_id, request.message)

        return ChatResponse(
            session_id=session_id,
            message=response['message'],
            type=response['type'],
            intent=response.get('intent')
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

# è¿è¡Œ: uvicorn app:app --host 0.0.0.0 --port 8000
```

#### Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# ç´¢å¼•æ•°æ®ï¼ˆæ„å»ºæ—¶ï¼‰
RUN python indexer.py

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./chroma_db:/app/chroma_db
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=customer_service
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

---

### 1.6 ç›‘æ§ä¸ä¼˜åŒ–

#### Prometheus Metrics

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge
from functools import wraps
import time

# å®šä¹‰metrics
REQUEST_COUNT = Counter(
    'chatbot_requests_total',
    'Total requests',
    ['intent', 'type']
)

RESPONSE_TIME = Histogram(
    'chatbot_response_seconds',
    'Response time',
    ['intent']
)

ACTIVE_SESSIONS = Gauge(
    'chatbot_active_sessions',
    'Active chat sessions'
)

def track_request(func):
    """è£…é¥°å™¨ï¼šè·Ÿè¸ªè¯·æ±‚"""
    @wraps(func)
    async def wrapper(request: ChatRequest):
        start_time = time.time()

        response = await func(request)

        # è®°å½•metrics
        REQUEST_COUNT.labels(
            intent=response.intent['intent'] if response.intent else 'unknown',
            type=response.type
        ).inc()

        RESPONSE_TIME.labels(
            intent=response.intent['intent'] if response.intent else 'unknown'
        ).observe(time.time() - start_time)

        return response

    return wrapper

# åº”ç”¨åˆ°API
@app.post("/chat", response_model=ChatResponse)
@track_request
async def chat(request: ChatRequest):
    # ... (åŒå‰é¢çš„å®ç°)
```

#### æ€§èƒ½ä¼˜åŒ–

```python
# optimization.py
from functools import lru_cache
import redis
import hashlib
import json

class CachedQA:
    """å¸¦ç¼“å­˜çš„QAç³»ç»Ÿ"""

    def __init__(self, qa_engine, redis_client=None):
        self.qa_engine = qa_engine
        self.redis = redis_client or redis.Redis(host='localhost', port=6379)

    def _cache_key(self, question: str) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        return f"qa:{hashlib.md5(question.encode()).hexdigest()}"

    def query(self, question: str):
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        cache_key = self._cache_key(question)

        # æ£€æŸ¥ç¼“å­˜
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # æŸ¥è¯¢
        result = self.qa_engine.query(question)

        # ç¼“å­˜ç»“æœï¼ˆ1å°æ—¶ï¼‰
        self.redis.setex(
            cache_key,
            3600,
            json.dumps(result, ensure_ascii=False)
        )

        return result
```

---

### 1.7 æ•ˆæœè¯„ä¼°

#### ç¦»çº¿è¯„ä¼°

```python
# evaluation.py
from typing import List, Dict
import json

class ChatbotEvaluator:
    """èŠå¤©æœºå™¨äººè¯„ä¼°"""

    def __init__(self, test_qa_pairs: List[Dict]):
        """
        test_qa_pairs: [{"question": "...", "expected_answer": "...", "category": "..."}]
        """
        self.test_pairs = test_qa_pairs

    def evaluate(self, qa_engine):
        """è¯„ä¼°QAå¼•æ“"""
        results = {
            'total': len(self.test_pairs),
            'correct': 0,
            'by_category': {}
        }

        for pair in self.test_pairs:
            question = pair['question']
            expected = pair['expected_answer']
            category = pair.get('category', 'general')

            # è·å–å›ç­”
            response = qa_engine.query(question)
            answer = response['answer']

            # åˆ¤æ–­æ­£ç¡®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼šå…³é”®è¯åŒ¹é…ï¼‰
            is_correct = self._check_correctness(answer, expected)

            if is_correct:
                results['correct'] += 1

            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            if category not in results['by_category']:
                results['by_category'][category] = {'total': 0, 'correct': 0}

            results['by_category'][category]['total'] += 1
            if is_correct:
                results['by_category'][category]['correct'] += 1

        # è®¡ç®—å‡†ç¡®ç‡
        results['accuracy'] = results['correct'] / results['total']

        for cat, stats in results['by_category'].items():
            stats['accuracy'] = stats['correct'] / stats['total']

        return results

    def _check_correctness(self, answer: str, expected: str) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æå–å…³é”®è¯
        expected_keywords = set(expected.lower().split())
        answer_keywords = set(answer.lower().split())

        # å…³é”®è¯è¦†ç›–ç‡
        overlap = len(expected_keywords & answer_keywords)
        coverage = overlap / len(expected_keywords) if expected_keywords else 0

        return coverage > 0.5  # 50%çš„å…³é”®è¯åŒ¹é…å³è®¤ä¸ºæ­£ç¡®

# ä½¿ç”¨
test_pairs = [
    {
        "question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
        "expected_answer": "åœ¨è®¾ç½®é¡µé¢ç‚¹å‡»å¿˜è®°å¯†ç ï¼Œè¾“å…¥é‚®ç®±æ¥æ”¶é‡ç½®é“¾æ¥",
        "category": "è´¦æˆ·ç®¡ç†"
    },
    # ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
]

evaluator = ChatbotEvaluator(test_pairs)
qa_engine = CustomerServiceQA()
results = evaluator.evaluate(qa_engine)

print(f"æ€»ä½“å‡†ç¡®ç‡: {results['accuracy']:.2%}")
for cat, stats in results['by_category'].items():
    print(f"{cat}: {stats['accuracy']:.2%}")
```

---

### 1.8 æŒç»­è¿­ä»£

**è¿­ä»£è®¡åˆ’ï¼š**

1. **Week 1-2: MVPç‰ˆæœ¬**
   - åŸºç¡€RAGé—®ç­”
   - ç®€å•Webç•Œé¢
   - æ ¸å¿ƒåŠŸèƒ½å¯ç”¨

2. **Week 3-4: ä¼˜åŒ–**
   - æ·»åŠ æ„å›¾è¯†åˆ«
   - å¤šè½®å¯¹è¯æ”¯æŒ
   - æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ï¼‰

3. **Week 5-6: ç”Ÿäº§åŒ–**
   - Dockeréƒ¨ç½²
   - ç›‘æ§å‘Šè­¦
   - æ•°æ®å¤‡ä»½

4. **Week 7+: æŒç»­æ”¹è¿›**
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - åˆ†æå¤±è´¥æ¡ˆä¾‹
   - å®šæœŸæ›´æ–°çŸ¥è¯†åº“

---

## é¡¹ç›®æ¡ˆä¾‹2ï¼šä»£ç åŠ©æ‰‹ï¼ˆç±»Copilotï¼‰

> **ç›®æ ‡ï¼š** æ„å»ºä¸€ä¸ªèƒ½è¾…åŠ©ç¼–ç¨‹çš„ä»£ç åŠ©æ‰‹ï¼Œæ”¯æŒä»£ç è¡¥å…¨ã€è§£é‡Šã€é‡æ„

### 2.1 æ ¸å¿ƒåŠŸèƒ½

**åŠŸèƒ½åˆ—è¡¨ï¼š**
1. **ä»£ç è¡¥å…¨** - æ ¹æ®ä¸Šä¸‹æ–‡è¡¥å…¨ä»£ç 
2. **ä»£ç è§£é‡Š** - è§£é‡Šä»£ç åŠŸèƒ½
3. **ä»£ç å®¡æŸ¥** - å‘ç°æ½œåœ¨é—®é¢˜
4. **ä»£ç é‡æ„** - ä¼˜åŒ–ä»£ç ç»“æ„
5. **å•å…ƒæµ‹è¯•ç”Ÿæˆ** - è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•

### 2.2 æŠ€æœ¯æ¶æ„ï¼ˆç²¾ç®€ç‰ˆï¼‰

```python
# code_assistant.py
from transformers import AutoModelForCausalLM, AutoTokenizer

class CodeAssistant:
    """ä»£ç åŠ©æ‰‹"""

    def __init__(self, model_name="deepseek-ai/deepseek-coder-6.7b-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

    def complete_code(self, prefix: str, max_length: int = 100):
        """ä»£ç è¡¥å…¨"""
        inputs = self.tokenizer(prefix, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_length=len(inputs['input_ids'][0]) + max_length,
            temperature=0.2,
            top_p=0.95,
            do_sample=True
        )

        completion = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return completion[len(prefix):]

    def explain_code(self, code: str):
        """è§£é‡Šä»£ç """
        prompt = f"""è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½ï¼š

```python
{code}
```

è§£é‡Šï¼š"""
        # ä½¿ç”¨LLMç”Ÿæˆè§£é‡Š
        return self._generate(prompt)

    def review_code(self, code: str):
        """ä»£ç å®¡æŸ¥"""
        prompt = f"""è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼ŒæŒ‡å‡ºæ½œåœ¨é—®é¢˜ï¼š

```python
{code}
```

å®¡æŸ¥æ„è§ï¼š"""
        return self._generate(prompt)

    def _generate(self, prompt: str, max_length: int = 500):
        """é€šç”¨ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            top_p=0.95
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

# VSCodeæ‰©å±•é›†æˆï¼ˆæ¦‚å¿µï¼‰
# - ä½¿ç”¨Language Server Protocol (LSP)
# - ç›‘å¬ç¼–è¾‘å™¨äº‹ä»¶
# - è°ƒç”¨CodeAssistant API
```

### 2.3 å…³é”®å®ç°è¦ç‚¹

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- ä½¿ç”¨å°æ¨¡å‹ï¼ˆ1B-7Bï¼‰ä¿è¯å“åº”é€Ÿåº¦
- ä»£ç è¡¥å…¨å»¶è¿Ÿ<100ms
- æœ¬åœ°éƒ¨ç½²é¿å…ç½‘ç»œå»¶è¿Ÿ

**ä»£ç ç†è§£ï¼š**
- ä½¿ç”¨ASTè§£æä»£ç ç»“æ„
- æå–å‡½æ•°ç­¾åã€ç±»å®šä¹‰
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¥å…¨

**è¯„ä¼°æŒ‡æ ‡ï¼š**
- Pass@Kï¼ˆç”ŸæˆKä¸ªå€™é€‰ä¸­è‡³å°‘1ä¸ªæ­£ç¡®ï¼‰
- ä»£ç ç›¸ä¼¼åº¦ï¼ˆç¼–è¾‘è·ç¦»ï¼‰
- ç”¨æˆ·æ¥å—ç‡

---

## é¡¹ç›®æ¡ˆä¾‹3ï¼šå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå¹³å°

> **ç›®æ ‡ï¼š** æ„å»ºä¸€ä¸ªæ”¯æŒæ–‡ç”Ÿå›¾ã€å›¾ç”Ÿæ–‡ã€å›¾åƒç¼–è¾‘çš„å¤šæ¨¡æ€å¹³å°

### 3.1 æ ¸å¿ƒåŠŸèƒ½

1. **æ–‡ç”Ÿå›¾** - æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒ
2. **å›¾ç”Ÿæ–‡** - å›¾åƒæè¿°ç”Ÿæˆ
3. **å›¾åƒç¼–è¾‘** - ControlNetå¼•å¯¼ç¼–è¾‘
4. **é£æ ¼è¿ç§»** - è‰ºæœ¯é£æ ¼è½¬æ¢

### 3.2 æŠ€æœ¯æ¶æ„ï¼ˆç²¾ç®€ç‰ˆï¼‰

```python
# multimodal_platform.py
from diffusers import StableDiffusionPipeline, ControlNetModel
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch

class MultimodalPlatform:
    """å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå¹³å°"""

    def __init__(self):
        # æ–‡ç”Ÿå›¾
        self.sd_pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16
        ).to("cuda")

        # å›¾ç”Ÿæ–‡
        self.blip_processor = BlipProcessor.from_pretrained(
            "Salesforce/blip-image-captioning-base"
        )
        self.blip_model = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-base"
        ).to("cuda")

    def text_to_image(self, prompt: str, negative_prompt: str = ""):
        """æ–‡ç”Ÿå›¾"""
        image = self.sd_pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=50,
            guidance_scale=7.5
        ).images[0]

        return image

    def image_to_text(self, image):
        """å›¾ç”Ÿæ–‡"""
        inputs = self.blip_processor(image, return_tensors="pt").to("cuda")
        outputs = self.blip_model.generate(**inputs, max_length=50)
        caption = self.blip_processor.decode(outputs[0], skip_special_tokens=True)

        return caption

# Webç•Œé¢ï¼ˆGradioï¼‰
import gradio as gr

platform = MultimodalPlatform()

def generate_image(prompt, negative_prompt):
    return platform.text_to_image(prompt, negative_prompt)

def caption_image(image):
    return platform.image_to_text(image)

with gr.Blocks() as demo:
    with gr.Tab("æ–‡ç”Ÿå›¾"):
        prompt_input = gr.Textbox(label="æè¿°")
        negative_input = gr.Textbox(label="è´Ÿé¢æç¤ºï¼ˆå¯é€‰ï¼‰")
        image_output = gr.Image(label="ç”Ÿæˆå›¾åƒ")
        gen_btn = gr.Button("ç”Ÿæˆ")
        gen_btn.click(generate_image, [prompt_input, negative_input], image_output)

    with gr.Tab("å›¾ç”Ÿæ–‡"):
        image_input = gr.Image(label="ä¸Šä¼ å›¾åƒ")
        caption_output = gr.Textbox(label="å›¾åƒæè¿°")
        cap_btn = gr.Button("ç”Ÿæˆæè¿°")
        cap_btn.click(caption_image, image_input, caption_output)

demo.launch()
```

### 3.3 å…³é”®å®ç°è¦ç‚¹

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- ä½¿ç”¨FP16æ··åˆç²¾åº¦
- æ‰¹å¤„ç†å¤šä¸ªè¯·æ±‚
- GPUæ˜¾å­˜ç®¡ç†

**è´¨é‡æ§åˆ¶ï¼š**
- Negative Promptè¿‡æ»¤ä¸è‰¯å†…å®¹
- å®‰å…¨åˆ†ç±»å™¨æ£€æµ‹NSFW
- æ°´å°æ·»åŠ 

---

## é€šç”¨æœ€ä½³å®è·µ

### é¡¹ç›®å¼€å‘æµç¨‹

```
1. éœ€æ±‚åˆ†æ
   â”œâ”€ æ˜ç¡®åŠŸèƒ½éœ€æ±‚
   â”œâ”€ ç¡®å®šæ€§èƒ½æŒ‡æ ‡
   â””â”€ è¯„ä¼°æŠ€æœ¯å¯è¡Œæ€§

2. MVPå¼€å‘ï¼ˆ1-2å‘¨ï¼‰
   â”œâ”€ æ ¸å¿ƒåŠŸèƒ½å®ç°
   â”œâ”€ ç®€å•UI
   â””â”€ åŸºç¡€æµ‹è¯•

3. è¿­ä»£ä¼˜åŒ–ï¼ˆ2-4å‘¨ï¼‰
   â”œâ”€ æ€§èƒ½ä¼˜åŒ–
   â”œâ”€ åŠŸèƒ½å®Œå–„
   â””â”€ ç”¨æˆ·æµ‹è¯•

4. ç”Ÿäº§éƒ¨ç½²ï¼ˆ1-2å‘¨ï¼‰
   â”œâ”€ DockeråŒ–
   â”œâ”€ CI/CD
   â””â”€ ç›‘æ§å‘Šè­¦

5. æŒç»­è¿è¥
   â”œâ”€ æ”¶é›†åé¦ˆ
   â”œâ”€ æ•°æ®åˆ†æ
   â””â”€ ç‰ˆæœ¬è¿­ä»£
```

### ä»£ç ç»„ç»‡

```
project/
â”œâ”€â”€ data/                  # æ•°æ®
â”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/        # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ vector_db/        # å‘é‡æ•°æ®åº“
â”œâ”€â”€ src/                   # æºä»£ç 
â”‚   â”œâ”€â”€ data/             # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ models/           # æ¨¡å‹
â”‚   â”œâ”€â”€ api/              # APIæœåŠ¡
â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°
â”œâ”€â”€ tests/                 # æµ‹è¯•
â”œâ”€â”€ configs/              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ scripts/              # è„šæœ¬
â”œâ”€â”€ docs/                 # æ–‡æ¡£
â”œâ”€â”€ requirements.txt      # ä¾èµ–
â”œâ”€â”€ Dockerfile            # Dockeré…ç½®
â””â”€â”€ README.md            # è¯´æ˜æ–‡æ¡£
```

### æ–‡æ¡£è§„èŒƒ

**READMEå¿…å¤‡å†…å®¹ï¼š**
1. é¡¹ç›®ç®€ä»‹
2. åŠŸèƒ½ç‰¹æ€§
3. å¿«é€Ÿå¼€å§‹
4. å®‰è£…æŒ‡å—
5. ä½¿ç”¨ç¤ºä¾‹
6. APIæ–‡æ¡£
7. é…ç½®è¯´æ˜
8. å¸¸è§é—®é¢˜
9. è´¡çŒ®æŒ‡å—
10. è®¸å¯è¯

---

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: å¦‚ä½•æ§åˆ¶APIæˆæœ¬ï¼Ÿ

**è§£å†³æ–¹æ¡ˆï¼š**
1. **ç¼“å­˜ç­–ç•¥** - ç›¸åŒè¯·æ±‚ç¼“å­˜ç»“æœ
2. **æ¨¡å‹é€‰æ‹©** - ç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹
3. **æ‰¹å¤„ç†** - åˆå¹¶å¤šä¸ªè¯·æ±‚
4. **ç”¨é‡ç›‘æ§** - è®¾ç½®å‘Šè­¦é˜ˆå€¼
5. **Promptä¼˜åŒ–** - å‡å°‘tokenä½¿ç”¨

### Q2: å¦‚ä½•æå‡å‡†ç¡®ç‡ï¼Ÿ

**æ–¹æ³•ï¼š**
1. **æ•°æ®è´¨é‡** - æ¸…æ´—ã€å»é‡ã€æ ‡æ³¨
2. **Promptä¼˜åŒ–** - Few-shotã€CoT
3. **RAGä¼˜åŒ–** - æ”¹è¿›æ£€ç´¢ã€chunking
4. **æ¨¡å‹å¾®è°ƒ** - LoRAå¾®è°ƒç‰¹å®šé¢†åŸŸ
5. **é›†æˆå­¦ä¹ ** - å¤šæ¨¡å‹æŠ•ç¥¨

### Q3: å¦‚ä½•å¤„ç†é•¿æ–‡æ¡£ï¼Ÿ

**ç­–ç•¥ï¼š**
1. **åˆ†å—å¤„ç†** - æ™ºèƒ½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰
2. **Map-Reduce** - å…ˆæ€»ç»“å†åˆå¹¶
3. **é•¿ä¸Šä¸‹æ–‡æ¨¡å‹** - ä½¿ç”¨Claude/GPT-4-32k
4. **å±‚æ¬¡æ£€ç´¢** - ç²—æ£€ç´¢+ç²¾æ£€ç´¢

### Q4: å¦‚ä½•ä¿è¯å“åº”é€Ÿåº¦ï¼Ÿ

**ä¼˜åŒ–ï¼š**
1. **ç¼“å­˜** - Redisç¼“å­˜çƒ­é—¨è¯·æ±‚
2. **å¼‚æ­¥å¤„ç†** - ä½¿ç”¨é˜Ÿåˆ—å¤„ç†æ…¢ä»»åŠ¡
3. **CDN** - é™æ€èµ„æºä½¿ç”¨CDN
4. **æ•°æ®åº“ä¼˜åŒ–** - ç´¢å¼•ã€è¿æ¥æ± 
5. **é™æµ** - é˜²æ­¢è¿‡è½½

---

## æ€»ç»“

**å…³é”®è¦ç‚¹ï¼š**

1. **ä»MVPå¼€å§‹** - å¿«é€ŸéªŒè¯ï¼Œé€æ­¥è¿­ä»£
2. **é‡è§†æ•°æ®è´¨é‡** - æ•°æ®å†³å®šæ•ˆæœä¸Šé™
3. **ç›‘æ§å…ˆè¡Œ** - æå‰å‘ç°é—®é¢˜
4. **æˆæœ¬æ„è¯†** - APIè´¹ç”¨ã€æœåŠ¡å™¨æˆæœ¬
5. **ç”¨æˆ·åé¦ˆ** - æŒç»­æ”¹è¿›

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š**

1. é€‰æ‹©ä¸€ä¸ªé¡¹ç›®æ¡ˆä¾‹å¼€å§‹å®è·µ
2. æŒ‰ç…§æ•™ç¨‹ä¸€æ­¥æ­¥å®ç°
3. é‡åˆ°é—®é¢˜æŸ¥é˜…å¯¹åº”æ–‡æ¡£
4. å®Œæˆåå‡†å¤‡æ¼”ç¤ºææ–™

**èµ„æºé“¾æ¥ï¼š**
- [æ•°æ®å·¥ç¨‹](./13_Data_Engineering.md)
- [MLOps](./14_MLOps_Best_Practices.md)
- [RAGç³»ç»Ÿ](./03_RAG_System_Theory.md)
- [ç”Ÿäº§éƒ¨ç½²](./10_Production_Deployment.md)

---

**ç¥ä½ é¡¹ç›®å¼€å‘é¡ºåˆ©ï¼è®°ä½ï¼šæœ€å¥½çš„å­¦ä¹ æ–¹å¼æ˜¯åŠ¨æ‰‹å®è·µã€‚ğŸš€**

**æœ€åæ›´æ–°ï¼š** 2025-12-02
