# Prompt Engineeringç†è®ºä¸å®è·µ

## ç›®å½•
1. [Promptå·¥ç¨‹åŸºç¡€](#promptå·¥ç¨‹åŸºç¡€)
2. [æ ¸å¿ƒæŠ€æœ¯](#æ ¸å¿ƒæŠ€æœ¯)
3. [é«˜çº§ç­–ç•¥](#é«˜çº§ç­–ç•¥)
4. [ä¼˜åŒ–ä¸è¯„ä¼°](#ä¼˜åŒ–ä¸è¯„ä¼°)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## Promptå·¥ç¨‹åŸºç¡€

### ä»€ä¹ˆæ˜¯Prompt Engineeringï¼Ÿ

**å®šä¹‰ï¼š** è®¾è®¡å’Œä¼˜åŒ–è¾“å…¥æ–‡æœ¬ï¼Œä»¥å¼•å¯¼LLMç”ŸæˆæœŸæœ›çš„è¾“å‡ºã€‚

```
Bad Prompt:  "è§£é‡ŠAI"
Good Prompt: "è¯·ç”¨300å­—å·¦å³ï¼Œé€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œå‘ä¸€ä¸ªé«˜ä¸­ç”Ÿè§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Œ
             åŒ…æ‹¬å®šä¹‰ã€ä¸»è¦åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚"
```

### ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

LLMçš„è¡Œä¸ºé«˜åº¦ä¾èµ–promptï¼š

```python
# åŒä¸€ä¸ªé—®é¢˜ï¼Œä¸åŒpromptï¼Œç»“æœå¤©å·®åœ°åˆ«
prompt1 = "2+2ç­‰äºå¤šå°‘ï¼Ÿ"
# â†’ "4"

prompt2 = "ä½ æ˜¯ä¸€ä¸ªè¯—äººã€‚2+2ç­‰äºå¤šå°‘ï¼Ÿ"
# â†’ "å››å­£è½®å›ï¼ŒäºŒåŠ äºŒï¼ŒåŒ–ä½œæ˜¥ä¹‹å››å­£..."

prompt3 = "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦æ•™æˆã€‚è¯¦ç»†è§£é‡Š2+2ä¸ºä»€ä¹ˆç­‰äº4ã€‚"
# â†’ "æ ¹æ®çš®äºšè¯ºå…¬ç†ï¼Œ2å®šä¹‰ä¸º1çš„åç»§çš„åç»§..."
```

### Promptçš„è§£å‰–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [è§’è‰²] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå·¥ç¨‹å¸ˆ        â”‚  â† Role
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [èƒŒæ™¯] ç”¨æˆ·æ­£åœ¨å­¦ä¹ æ•°æ®ç»“æ„              â”‚  â† Context
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ä»»åŠ¡] è¯·è§£é‡ŠäºŒå‰æœç´¢æ ‘çš„æŸ¥æ‰¾ç®—æ³•        â”‚  â† Task
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [æ ¼å¼] ç”¨ä»£ç +æ³¨é‡Šçš„æ–¹å¼å±•ç¤º             â”‚  â† Format
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [çº¦æŸ] ä»£ç ä¸è¶…è¿‡50è¡Œ                   â”‚  â† Constraints
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ç¤ºä¾‹] è¾“å…¥: [5,3,7,1,9]               â”‚  â† Examples
â”‚         è¾“å‡º: æ‰¾åˆ°7ï¼Œè·¯å¾„5â†’7             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ ¸å¿ƒæŠ€æœ¯

### 1. Zero-Shot Prompting

ç›´æ¥æé—®ï¼Œä¸æä¾›ç¤ºä¾‹ï¼š

```python
prompt = """
å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿï¼š
æ–‡æœ¬ï¼šè¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼

æƒ…æ„Ÿï¼š
"""
```

**é€‚ç”¨åœºæ™¯ï¼š**
- ç®€å•ä»»åŠ¡
- é€šç”¨çŸ¥è¯†
- æ¨¡å‹èƒ½åŠ›å¼ºï¼ˆGPT-4ï¼‰

### 2. Few-Shot Prompting

æä¾›ç¤ºä¾‹æ¥æŒ‡å¯¼æ¨¡å‹ï¼š

```python
prompt = """
å°†æ–‡æœ¬åˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿï¼š

ç¤ºä¾‹1ï¼š
æ–‡æœ¬ï¼šè¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œæˆ‘éå¸¸å–œæ¬¢ï¼
æƒ…æ„Ÿï¼šæ­£é¢

ç¤ºä¾‹2ï¼š
æ–‡æœ¬ï¼šè´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸æ¨èã€‚
æƒ…æ„Ÿï¼šè´Ÿé¢

ç¤ºä¾‹3ï¼š
æ–‡æœ¬ï¼šè¿˜å¯ä»¥ï¼Œä»·æ ¼åˆç†ã€‚
æƒ…æ„Ÿï¼šæ­£é¢

ç°åœ¨åˆ†ç±»è¿™ä¸ªï¼š
æ–‡æœ¬ï¼šä½“éªŒå¾ˆç³Ÿç³•ï¼Œæµªè´¹é’±ã€‚
æƒ…æ„Ÿï¼š
"""
```

**Few-Shotçš„é»„é‡‘æ³•åˆ™ï¼š**
- **æ•°é‡ï¼š** 3-5ä¸ªç¤ºä¾‹æœ€ä½³
- **å¤šæ ·æ€§ï¼š** è¦†ç›–ä¸åŒåœºæ™¯
- **è´¨é‡ï¼š** ç¤ºä¾‹è¦å‡†ç¡®
- **é¡ºåºï¼š** æœ€è¿‘çš„ç¤ºä¾‹å½±å“æœ€å¤§

### 3. Chain-of-Thought (CoT)

å¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†ï¼š

**æ ‡å‡†Promptï¼š**
```
é—®ï¼šä¸€ä¸ªç¯®å­é‡Œæœ‰15ä¸ªè‹¹æœï¼Œæ‹¿èµ°3ä¸ªï¼Œåˆæ”¾å›2ä¸ªï¼Œç°åœ¨æœ‰å¤šå°‘ä¸ªï¼Ÿ
ç­”ï¼š14ä¸ª
```

**CoT Promptï¼š**
```
é—®ï¼šä¸€ä¸ªç¯®å­é‡Œæœ‰15ä¸ªè‹¹æœï¼Œæ‹¿èµ°3ä¸ªï¼Œåˆæ”¾å›2ä¸ªï¼Œç°åœ¨æœ‰å¤šå°‘ä¸ªï¼Ÿ
ç­”ï¼šè®©æˆ‘ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. æœ€åˆæœ‰15ä¸ªè‹¹æœ
2. æ‹¿èµ°3ä¸ªåï¼š15 - 3 = 12ä¸ª
3. æ”¾å›2ä¸ªåï¼š12 + 2 = 14ä¸ª
å› æ­¤ï¼Œç°åœ¨æœ‰14ä¸ªè‹¹æœã€‚
```

**å…³é”®çŸ­è¯­ï¼š**
- "Let's think step by step"
- "è®©æˆ‘ä»¬ä¸€æ­¥æ­¥åˆ†æ"
- "First, ...; Second, ...; Therefore, ..."

**Zero-Shot CoTï¼š**
```python
prompt = f"""
{question}

Let's think step by step.
"""
```

### 4. Self-Consistency

ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼ŒæŠ•ç¥¨é€‰æ‹©ï¼š

```python
def self_consistency(question, n=5):
    answers = []
    for _ in range(n):
        prompt = f"{question}\nLet's think step by step."
        answer = llm(prompt, temperature=0.7)  # å¢åŠ éšæœºæ€§
        answers.append(extract_final_answer(answer))

    # å¤šæ•°æŠ•ç¥¨
    from collections import Counter
    most_common = Counter(answers).most_common(1)[0][0]
    return most_common

# ç¤ºä¾‹
question = "If a train travels 120 km in 2 hours, what's its speed?"
final_answer = self_consistency(question, n=5)
# å³ä½¿æœ‰1-2æ¬¡é”™è¯¯ï¼Œå¤šæ•°æŠ•ç¥¨ä»èƒ½å¾—åˆ°æ­£ç¡®ç­”æ¡ˆ
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤æ‚æ¨ç†
- é«˜å‡†ç¡®ç‡è¦æ±‚
- å¯ä»¥æ‰¿å—å¤šæ¬¡APIè°ƒç”¨æˆæœ¬

### 5. Retrieval-Augmented Generation

ç»“åˆå¤–éƒ¨çŸ¥è¯†ï¼ˆå·²åœ¨RAGç« èŠ‚è¯¦è¿°ï¼‰ï¼š

```python
prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£ï¼š
{retrieved_documents}

é—®é¢˜ï¼š{user_question}

ç­”æ¡ˆï¼š
"""
```

### 6. ReActï¼ˆReasoning + Actingï¼‰

äº¤æ›¿è¿›è¡Œæ¨ç†å’Œè¡ŒåŠ¨ï¼š

```
Thought 1: æˆ‘éœ€è¦æŸ¥æ‰¾Pythonçš„æœ€æ–°ç‰ˆæœ¬
Action 1: search["Python latest version 2024"]
Observation 1: Python 3.12.0 released in October 2023

Thought 2: ç”¨æˆ·é—®çš„æ˜¯2024å¹´ï¼Œæˆ‘éœ€è¦æ›´æ–°çš„ä¿¡æ¯
Action 2: search["Python releases 2024"]
Observation 2: Python 3.12.3 released in April 2024

Thought 3: ç°åœ¨æœ‰äº†ç­”æ¡ˆ
Answer: æˆªè‡³2024å¹´ï¼ŒPythonçš„æœ€æ–°ç‰ˆæœ¬æ˜¯3.12.3
```

---

## é«˜çº§ç­–ç•¥

### è§’è‰²æ‰®æ¼”ï¼ˆRole Promptingï¼‰

é€šè¿‡è§’è‰²è®¾å®šæ”¹å˜è¾“å‡ºé£æ ¼ï¼š

```python
# åŸºç¡€prompt
"è§£é‡Šé‡å­è®¡ç®—"

# è§’è‰²1ï¼šä¸“å®¶
"""
ä½ æ˜¯ä¸€ä½é‡å­ç‰©ç†å­¦æ•™æˆã€‚
ç”¨å­¦æœ¯ä¸¥è°¨çš„è¯­è¨€è§£é‡Šé‡å­è®¡ç®—ã€‚
"""

# è§’è‰²2ï¼šè€å¸ˆ
"""
ä½ æ˜¯ä¸€ä½ç»™å°å­¦ç”Ÿä¸Šè¯¾çš„è€å¸ˆã€‚
ç”¨ç®€å•æœ‰è¶£çš„æ¯”å–»è§£é‡Šé‡å­è®¡ç®—ã€‚
"""

# è§’è‰²3ï¼šå·¥ç¨‹å¸ˆ
"""
ä½ æ˜¯ä¸€ä½é‡å­è®¡ç®—å·¥ç¨‹å¸ˆã€‚
ä»å®é™…åº”ç”¨è§’åº¦è§£é‡Šé‡å­è®¡ç®—ã€‚
"""
```

**æ•ˆæœï¼š**
- ä¸“å®¶ï¼šä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä¸¥è°¨å‡†ç¡®
- è€å¸ˆï¼šç”¨æ¯”å–»ï¼Œé€šä¿—æ˜“æ‡‚
- å·¥ç¨‹å¸ˆï¼šå…³æ³¨åº”ç”¨ï¼Œå®ç”¨ä¸»ä¹‰

### æ ¼å¼æ§åˆ¶

**JSONè¾“å‡ºï¼š**
```python
prompt = """
åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿå’Œä¸»é¢˜ï¼š
è¯„è®ºï¼š"{review}"

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
  "sentiment": "positive/negative/neutral",
  "score": 0.0-1.0,
  "topics": ["topic1", "topic2"],
  "summary": "ä¸€å¥è¯æ€»ç»“"
}

ä»…è¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
```

**è¡¨æ ¼è¾“å‡ºï¼š**
```python
prompt = """
å°†ä»¥ä¸‹ä¿¡æ¯æ•´ç†æˆMarkdownè¡¨æ ¼ï¼š

ä¿¡æ¯ï¼š{data}

è¡¨æ ¼æ ¼å¼ï¼š
| å§“å | å¹´é¾„ | èŒä¸š |
|-----|------|------|
| ... | ...  | ...  |
"""
```

### è´Ÿé¢æç¤ºï¼ˆNegative Promptingï¼‰

æ˜ç¡®ä¸æƒ³è¦çš„å†…å®¹ï¼š

```python
prompt = """
å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« ã€‚

è¦æ±‚ï¼š
âœ“ 500å­—å·¦å³
âœ“ åŒ…å«å…·ä½“ä¾‹å­
âœ“ é¢å‘æ™®é€šè¯»è€…

ä¸è¦ï¼š
âœ— ä¸è¦ä½¿ç”¨è¿‡äºæŠ€æœ¯åŒ–çš„æœ¯è¯­
âœ— ä¸è¦è¶…è¿‡600å­—
âœ— ä¸è¦åŒ…å«ä»£ç 
"""
```

### æ€ç»´æ ‘ï¼ˆTree of Thoughtï¼‰

æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„ï¼š

```
Question: How to improve code quality?

Path 1: Testing perspective
  â†’ Unit tests
  â†’ Integration tests
  â†’ Code coverage
  Evaluation: Good, but incomplete

Path 2: Design perspective
  â†’ SOLID principles
  â†’ Design patterns
  â†’ Code review
  Evaluation: Strong foundation

Path 3: Process perspective
  â†’ CI/CD
  â†’ Automated linting
  â†’ Documentation
  Evaluation: Important but not core

Best path: Combine Path 2 (foundation) + Path 1 (validation)
```

### Meta-Prompting

ç”¨LLMç”ŸæˆPromptï¼š

```python
meta_prompt = """
æˆ‘éœ€è¦ä¸€ä¸ªpromptæ¥å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
{task_description}

ç›®æ ‡ç”¨æˆ·ï¼š{target_audience}
æœŸæœ›è¾“å‡ºï¼š{expected_output}

è¯·è®¾è®¡ä¸€ä¸ªè¯¦ç»†çš„ã€ç»“æ„åŒ–çš„promptã€‚
"""

# LLMç”Ÿæˆæœ€ä¼˜prompt
optimized_prompt = llm(meta_prompt)

# ä½¿ç”¨ç”Ÿæˆçš„prompt
result = llm(optimized_prompt)
```

---

## ä¼˜åŒ–ä¸è¯„ä¼°

### Promptè¿­ä»£ä¼˜åŒ–æµç¨‹

```
1. Baseline Prompt
   â†“
2. æµ‹è¯• â†’ æ”¶é›†å¤±è´¥æ¡ˆä¾‹
   â†“
3. åˆ†æå¤±è´¥åŸå› 
   â†“
4. æ”¹è¿›Prompt
   â†“
5. A/Bæµ‹è¯•
   â†“
6. éƒ¨ç½²æœ€ä½³ç‰ˆæœ¬
   â†“
7. æŒç»­ç›‘æ§ â†’ è¿”å›æ­¥éª¤2
```

### A/Bæµ‹è¯•æ¡†æ¶

```python
def ab_test_prompts(prompt_a, prompt_b, test_cases, n_runs=5):
    results_a = []
    results_b = []

    for test_case in test_cases:
        # æµ‹è¯•Prompt A
        for _ in range(n_runs):
            output_a = llm(prompt_a.format(**test_case))
            results_a.append(evaluate(output_a, test_case))

        # æµ‹è¯•Prompt B
        for _ in range(n_runs):
            output_b = llm(prompt_b.format(**test_case))
            results_b.append(evaluate(output_b, test_case))

    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    from scipy import stats
    t_stat, p_value = stats.ttest_ind(results_a, results_b)

    print(f"Prompt Aå¹³å‡åˆ†: {np.mean(results_a):.2f}")
    print(f"Prompt Bå¹³å‡åˆ†: {np.mean(results_b):.2f}")
    print(f"P-value: {p_value:.4f}")

    if p_value < 0.05:
        winner = "B" if np.mean(results_b) > np.mean(results_a) else "A"
        print(f"Prompt {winner}æ˜¾è‘—æ›´å¥½ (p < 0.05)")
    else:
        print("æ²¡æœ‰æ˜¾è‘—å·®å¼‚")
```

### è¯„ä¼°æŒ‡æ ‡

**è‡ªåŠ¨è¯„ä¼°ï¼š**

1. **ç²¾ç¡®åŒ¹é…**
   ```python
   accuracy = (predicted == ground_truth).mean()
   ```

2. **ROUGEï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰**
   ```python
   from rouge import Rouge
   rouge = Rouge()
   scores = rouge.get_scores(predicted, reference)
   ```

3. **BLEUï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰**
   ```python
   from nltk.translate.bleu_score import sentence_bleu
   score = sentence_bleu([reference], predicted)
   ```

**LLM-as-Judgeï¼ˆGPTè¯„ä¼°ï¼‰ï¼š**

```python
def llm_evaluate(question, answer):
    eval_prompt = f"""
    è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ï¼ˆ1-10åˆ†ï¼‰ï¼š

    é—®é¢˜ï¼š{question}
    ç­”æ¡ˆï¼š{answer}

    è¯„åˆ†æ ‡å‡†ï¼š
    - å‡†ç¡®æ€§ï¼ˆæ˜¯å¦æ­£ç¡®ï¼‰
    - å®Œæ•´æ€§ï¼ˆæ˜¯å¦å…¨é¢ï¼‰
    - æ¸…æ™°åº¦ï¼ˆæ˜¯å¦æ˜“æ‡‚ï¼‰

    è¯·ç»™å‡ºåˆ†æ•°å’Œç†ç”±ã€‚

    æ ¼å¼ï¼š
    åˆ†æ•°: X/10
    ç†ç”±: ...
    """

    evaluation = llm(eval_prompt)
    return parse_score(evaluation)
```

### å¸¸è§é—®é¢˜è¯Šæ–­

**é—®é¢˜1ï¼šè¾“å‡ºä¸ä¸€è‡´**

è§£å†³ï¼š
```python
# é™ä½temperature
llm(prompt, temperature=0.0)  # ç¡®å®šæ€§è¾“å‡º

# æˆ–ä½¿ç”¨self-consistency
```

**é—®é¢˜2ï¼šä¸éµå¾ªæ ¼å¼**

è§£å†³ï¼š
```python
# æ˜ç¡®æ ¼å¼è¦æ±‚
prompt = """
...

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
{
  "field1": "value1",
  "field2": "value2"
}

ç¤ºä¾‹è¾“å‡ºï¼š
{
  "sentiment": "positive",
  "score": 0.85
}

è¯·ä¸¥æ ¼æŒ‰ä¸Šè¿°JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
```

**é—®é¢˜3ï¼šå¹»è§‰ï¼ˆç¼–é€ ä¿¡æ¯ï¼‰**

è§£å†³ï¼š
```python
# æ·»åŠ çº¦æŸ
prompt = """
{question}

æ³¨æ„ï¼š
- ä»…åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´"ä¿¡æ¯ä¸è¶³ï¼Œæ— æ³•å›ç­”"
- ä¸è¦çŒœæµ‹æˆ–ç¼–é€ ä¿¡æ¯
"""
```

---

## æœ€ä½³å®è·µ

### é€šç”¨åŸåˆ™

**1. æ¸…æ™°æ˜ç¡®**
```
âŒ Bad:  "å†™ç‚¹å…³äºAIçš„ä¸œè¥¿"
âœ… Good: "å†™ä¸€ç¯‡300å­—çš„æ–‡ç« ï¼Œä»‹ç»AIåœ¨åŒ»ç–—é¢†åŸŸçš„3ä¸ªåº”ç”¨"
```

**2. æä¾›ä¸Šä¸‹æ–‡**
```
âŒ Bad:  "è¿™ä¸ªæ€ä¹ˆç”¨ï¼Ÿ"
âœ… Good: "åœ¨Pythonä¸­ï¼Œnumpy.array()å‡½æ•°å¦‚ä½•åˆ›å»ºäºŒç»´æ•°ç»„ï¼Ÿè¯·ç»™å‡ºç¤ºä¾‹ä»£ç ã€‚"
```

**3. åˆ†æ­¥æŒ‡å¯¼**
```
âœ… Good:
"è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤åˆ†æï¼š
1. æ€»ç»“æ–‡æœ¬ä¸»è¦å†…å®¹
2. æå–å…³é”®æ•°æ®ç‚¹
3. ç»™å‡º3æ¡æ´å¯Ÿ
4. æå‡ºæ”¹è¿›å»ºè®®"
```

**4. ä½¿ç”¨åˆ†éš”ç¬¦**
```python
prompt = """
åˆ†æä»¥ä¸‹ä»£ç ï¼š

```python
{code}
```

è¯·å›ç­”ï¼š
1. è¿™æ®µä»£ç çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ
2. æœ‰æ²¡æœ‰æ½œåœ¨çš„bugï¼Ÿ
3. å¦‚ä½•ä¼˜åŒ–ï¼Ÿ
"""
```

### é’ˆå¯¹ä¸åŒä»»åŠ¡çš„æ¨¡æ¿

**ä»£ç ç”Ÿæˆï¼š**
```python
TEMPLATE = """
è¯­è¨€ï¼š{language}
ä»»åŠ¡ï¼š{task}

è¦æ±‚ï¼š
- éµå¾ª{language}çš„æœ€ä½³å®è·µ
- åŒ…å«é”™è¯¯å¤„ç†
- æ·»åŠ ç±»å‹æ³¨è§£ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
- åŒ…å«docstring
- æä¾›ä½¿ç”¨ç¤ºä¾‹

ä»£ç ï¼š
"""
```

**ä»£ç å®¡æŸ¥ï¼š**
```python
TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„{language}å·¥ç¨‹å¸ˆã€‚è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼š

```{language}
{code}
```

ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. **æ­£ç¡®æ€§**ï¼šé€»è¾‘æ˜¯å¦æ­£ç¡®ï¼Ÿ
2. **æ€§èƒ½**ï¼šæœ‰æ— æ€§èƒ½é—®é¢˜ï¼Ÿæ—¶é—´/ç©ºé—´å¤æ‚åº¦ï¼Ÿ
3. **å®‰å…¨æ€§**ï¼šæœ‰æ— å®‰å…¨éšæ‚£ï¼ˆSQLæ³¨å…¥ã€XSSç­‰ï¼‰ï¼Ÿ
4. **å¯ç»´æŠ¤æ€§**ï¼šä»£ç æ˜¯å¦æ¸…æ™°ï¼Ÿæ˜¯å¦éµå¾ªæœ€ä½³å®è·µï¼Ÿ
5. **æ”¹è¿›å»ºè®®**ï¼šå…·ä½“çš„ä¼˜åŒ–æ–¹å‘

å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œè¯·æŒ‡å‡ºä»£ç è¡Œå·å¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ã€‚
"""
```

**æ–‡æœ¬åˆ†ç±»ï¼š**
```python
TEMPLATE = """
å°†ä»¥ä¸‹{data_type}åˆ†ç±»ä¸ºä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š
{categories}

ç¤ºä¾‹ï¼š
{examples}

å¾…åˆ†ç±»ï¼š
{input_text}

ç±»åˆ«ï¼š
"""
```

### ä¸åŒæ¨¡å‹çš„Promptå·®å¼‚

**GPT-4ï¼š**
- ç†è§£èƒ½åŠ›å¼ºï¼Œå¯ä»¥ç”¨å¤æ‚prompt
- éµå¾ªæŒ‡ä»¤å¥½ï¼Œæ ¼å¼æ§åˆ¶å¯é 
- å¯ä»¥å¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼ˆ32K tokensï¼‰

```python
# GPT-4å¯ä»¥å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡
prompt = """
1. åˆ†æè¿™æ®µä»£ç 
2. æ‰¾å‡ºæ‰€æœ‰bug
3. å¯¹æ¯ä¸ªbugï¼š
   a. è§£é‡Šä¸ºä»€ä¹ˆæ˜¯bug
   b. ç»™å‡ºä¿®å¤æ–¹æ¡ˆ
   c. æä¾›ä¿®å¤åçš„ä»£ç 
4. æ€»ç»“æ”¹è¿›è¦ç‚¹
"""
```

**GPT-3.5ï¼š**
- æˆæœ¬ä½ï¼Œé€Ÿåº¦å¿«
- éœ€è¦æ›´ç®€æ´çš„prompt
- ä¸Šä¸‹æ–‡çª—å£è¾ƒå°ï¼ˆ16Kï¼‰

```python
# GPT-3.5ç”¨ç®€åŒ–çš„prompt
prompt = """
æ‰¾å‡ºä»£ç ä¸­çš„bugå¹¶ä¿®å¤ï¼š

ä»£ç ï¼š
{code}

è¯·åˆ—å‡ºï¼š
1. Bugæè¿°
2. ä¿®å¤åçš„ä»£ç 
"""
```

**Claudeï¼š**
- æ“…é•¿é•¿æ–‡æœ¬åˆ†æ
- æ³¨é‡å®‰å…¨å’Œé“å¾·
- å–œæ¬¢è¯¦ç»†çš„ä¸Šä¸‹æ–‡

```python
# Claudeé€‚åˆè¯¦ç»†åˆ†æ
prompt = """
<context>
è¿™æ˜¯ä¸€ä¸ªé‡‘èäº¤æ˜“ç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å—...
</context>

<task>
è¯·åˆ†æä»£ç çš„å®‰å…¨æ€§ï¼Œç‰¹åˆ«å…³æ³¨ï¼š
- è¾“å…¥éªŒè¯
- æƒé™æ§åˆ¶
- æ•°æ®åŠ å¯†
</task>

<code>
{code}
</code>
"""
```

### æˆæœ¬ä¼˜åŒ–

**1. ç¼“å­˜å¸¸è§æŸ¥è¯¢**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_llm_call(prompt):
    return llm(prompt)
```

**2. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹**
```python
def smart_routing(task_complexity):
    if task_complexity == "simple":
        return gpt-3.5-turbo  # $0.001/1K tokens
    elif task_complexity == "medium":
        return gpt-4o-mini    # $0.0001/1K tokens
    else:
        return gpt-4          # $0.03/1K tokens
```

**3. å‹ç¼©ä¸Šä¸‹æ–‡**
```python
# æ€»ç»“æ£€ç´¢æ–‡æ¡£è€Œä¸æ˜¯å…¨éƒ¨ä¼ å…¥
def summarize_docs(docs):
    summary_prompt = f"æ€»ç»“ä»¥ä¸‹æ–‡æ¡£çš„å…³é”®ä¿¡æ¯ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š\n{docs}"
    return llm(summary_prompt)

compressed_context = summarize_docs(retrieved_docs)
final_prompt = f"åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ï¼š\n{compressed_context}\n\né—®é¢˜ï¼š{question}"
```

---

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šä»£ç é—®ç­”ç³»ç»ŸPrompt

```python
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹ã€‚ä½ çš„èŒè´£æ˜¯å¸®åŠ©å¼€å‘è€…ç†è§£ä»£ç åº“ã€‚

å›ç­”è§„èŒƒï¼š
1. åŸºäºæä¾›çš„ä»£ç ç‰‡æ®µå›ç­”ï¼Œä¸è¦çŒœæµ‹
2. å¼•ç”¨å…·ä½“çš„æ–‡ä»¶åå’Œè¡Œå·
3. æä¾›ä»£ç ç¤ºä¾‹æ—¶ï¼Œç¡®ä¿å¯è¿è¡Œ
4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡

å›ç­”ç»“æ„ï¼š
- ç›´æ¥å›ç­”é—®é¢˜
- å¼•ç”¨ç›¸å…³ä»£ç 
- æä¾›ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¦‚é€‚ç”¨ï¼‰
- æŒ‡å‡ºæ³¨æ„äº‹é¡¹ï¼ˆå¦‚é€‚ç”¨ï¼‰
"""

USER_PROMPT = """
ä»£ç ä¸Šä¸‹æ–‡ï¼š
{retrieved_code}

é—®é¢˜ï¼š{user_question}
"""
```

### æ¡ˆä¾‹2ï¼šè‡ªåŠ¨ä»£ç å®¡æŸ¥

```python
CODE_REVIEW_PROMPT = """
ä½œä¸ºä¸€ä¸ªä¸¥æ ¼çš„ä»£ç å®¡æŸ¥è€…ï¼Œå®¡æŸ¥ä»¥ä¸‹{language}ä»£ç ï¼š

{code}

å®¡æŸ¥æ¸…å•ï¼š

## 1. æ­£ç¡®æ€§
- [ ] é€»è¾‘æ­£ç¡®
- [ ] è¾¹ç•Œæ¡ä»¶å¤„ç†
- [ ] é”™è¯¯å¤„ç†å®Œå–„

## 2. æ€§èƒ½
- [ ] æ—¶é—´å¤æ‚åº¦åˆç†
- [ ] ç©ºé—´å¤æ‚åº¦ä¼˜åŒ–
- [ ] æ— ä¸å¿…è¦çš„å¾ªç¯/è®¡ç®—

## 3. å®‰å…¨æ€§
- [ ] è¾“å…¥éªŒè¯
- [ ] SQLæ³¨å…¥é˜²æŠ¤
- [ ] XSSé˜²æŠ¤

## 4. å¯ç»´æŠ¤æ€§
- [ ] å‘½åæ¸…æ™°
- [ ] æ³¨é‡Šå……åˆ†
- [ ] ç»“æ„åˆç†

å¯¹äºæ¯ä¸ªé—®é¢˜ï¼š
- æ ‡æ³¨ä¸¥é‡ç¨‹åº¦ï¼šğŸ”´ Critical / ğŸŸ¡ Warning / ğŸ”µ Info
- ç»™å‡ºå…·ä½“çš„ä»£ç ä½ç½®
- æä¾›ä¿®å¤å»ºè®®

æœ€åæ€»ç»“ï¼šå»ºè®®æ˜¯å¦æ‰¹å‡†æ­¤PRã€‚
"""
```

### æ¡ˆä¾‹3ï¼šPromptä¼˜åŒ–å™¨

```python
PROMPT_OPTIMIZER = """
ä½ æ˜¯ä¸€ä¸ªPromptå·¥ç¨‹ä¸“å®¶ã€‚æˆ‘æœ‰ä¸€ä¸ªæ•ˆæœä¸ä½³çš„promptï¼Œéœ€è¦ä½ ä¼˜åŒ–ã€‚

åŸå§‹Promptï¼š
{original_prompt}

é—®é¢˜æè¿°ï¼š
{issues}

ç›®æ ‡ï¼š
{goals}

è¯·æä¾›ä¼˜åŒ–åçš„promptï¼Œå¹¶è¯´æ˜ï¼š
1. åšäº†å“ªäº›æ”¹è¿›
2. ä¸ºä»€ä¹ˆè¿™äº›æ”¹è¿›ä¼šæœ‰æ•ˆ
3. é¢„æœŸæ•ˆæœæå‡

ä¼˜åŒ–åçš„Promptï¼š
"""
```

---

## å»¶ä¼¸é˜…è¯»

**è®ºæ–‡ï¼š**
1. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)
2. ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2022)
3. Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Yao et al., 2023)

**èµ„æºï¼š**
- OpenAI Prompt Engineering Guide
- Anthropic's Claude Prompt Library
- LangChain Prompt Templates
- Prompt Engineering Guide (dair-ai)

**å·¥å…·ï¼š**
- LangSmith: Promptç®¡ç†å’Œæµ‹è¯•
- PromptPerfect: è‡ªåŠ¨Promptä¼˜åŒ–
- Promptbase: Promptå¸‚åœº

**ä¸‹ä¸€æ­¥ï¼š**
- [å®Œæ•´å­¦ä¹ è·¯çº¿å›¾](05_Learning_Roadmap.md)
- [å®è·µé¡¹ç›®æŒ‡å—](../README.md)
