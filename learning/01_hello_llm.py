"""
ç¬¬ä¸€ä¸ªLLMå®è·µï¼šä½“éªŒè‡ªå›å½’ç”Ÿæˆ

è¿™ä¸ªè„šæœ¬å±•ç¤ºï¼š
1. LLMå¦‚ä½•ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå›å½’ï¼‰
2. ä¸ºä»€ä¹ˆç”Ÿæˆé•¿æ–‡æœ¬éœ€è¦æ—¶é—´
3. Temperatureå‚æ•°å¦‚ä½•å½±å“è¾“å‡º

ä½¿ç”¨å‰è¯·è®¾ç½®ï¼š
export OPENAI_API_KEY="your-key"
"""

from openai import OpenAI
import sys

def demo1_basic_generation():
    """æ¼”ç¤º1ï¼šåŸºç¡€æ–‡æœ¬ç”Ÿæˆ"""
    print("=" * 60)
    print("æ¼”ç¤º1ï¼šåŸºç¡€æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 60)

    client = OpenAI()

    prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥"
    print(f"\nè¾“å…¥: {prompt}")
    print(f"è¾“å‡º: ", end="", flush=True)

    # stream=True è®©æˆ‘ä»¬çœ‹åˆ°é€ä¸ªè¯çš„ç”Ÿæˆè¿‡ç¨‹
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        stream=True,  # å…³é”®å‚æ•°ï¼šæµå¼è¾“å‡º
        max_tokens=50
    )

    # é€ä¸ªæ‰“å°ç”Ÿæˆçš„è¯
    for chunk in response:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

    print("\n\nğŸ’¡ è§‚å¯Ÿï¼šæ‚¨çœ‹åˆ°äº†å—ï¼Ÿæ–‡å­—æ˜¯ä¸€ä¸ªä¸€ä¸ªè¹¦å‡ºæ¥çš„ï¼")
    print("   è¿™å°±æ˜¯'è‡ªå›å½’ç”Ÿæˆ' - LLMæ¯æ¬¡åªç”Ÿæˆä¸€ä¸ªè¯")


def demo2_temperature_effect():
    """æ¼”ç¤º2ï¼šTemperatureå‚æ•°çš„å½±å“"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º2ï¼šTemperatureå‚æ•°å¦‚ä½•å½±å“è¾“å‡º")
    print("=" * 60)

    client = OpenAI()
    prompt = "ä»Šå¤©å¤©æ°”çœŸ"

    print(f"\nè¾“å…¥: {prompt}")

    # æµ‹è¯•ä¸åŒçš„temperature
    temperatures = [0.0, 0.5, 1.0, 1.5]

    for temp in temperatures:
        print(f"\n--- Temperature = {temp} ---")
        print("è¾“å‡º: ", end="")

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=temp,
            max_tokens=20
        )

        print(response.choices[0].message.content)

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("   - Temperature = 0.0: æ¯æ¬¡è¾“å‡ºéƒ½ä¸€æ ·ï¼ˆç¡®å®šæ€§ï¼‰")
    print("   - Temperature = 0.5: æ¯”è¾ƒä¿å®ˆï¼Œåˆç†")
    print("   - Temperature = 1.0: æ ‡å‡†ï¼Œæœ‰ä¸€å®šåˆ›é€ æ€§")
    print("   - Temperature = 1.5: å¾ˆéšæœºï¼Œå¯èƒ½ä¸å¤ªåˆç†")


def demo3_attention_visualization():
    """æ¼”ç¤º3ï¼šç†è§£'æ³¨æ„åŠ›' - é€šè¿‡ä»»åŠ¡å±•ç¤º"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º3ï¼šç†è§£'æ³¨æ„åŠ›'æœºåˆ¶")
    print("=" * 60)

    client = OpenAI()

    # æµ‹è¯•ç”¨ä¾‹ï¼šéœ€è¦"å›å¤´çœ‹"æ‰èƒ½æ­£ç¡®å›ç­”
    test_cases = [
        {
            "prompt": "å°æ˜ä»Šå¤©å»è¶…å¸‚ä¹°äº†è‹¹æœã€‚ä»–å¾ˆå–œæ¬¢åƒæ°´æœã€‚é—®ï¼šè°å–œæ¬¢åƒæ°´æœï¼Ÿ",
            "explanation": "æ¨¡å‹éœ€è¦æ³¨æ„åˆ°'ä»–'æŒ‡çš„æ˜¯'å°æ˜'"
        },
        {
            "prompt": "çŒ«ååœ¨å«å­ä¸Šã€‚å®ƒæ˜¯é»‘è‰²çš„ã€‚é—®ï¼šä»€ä¹ˆæ˜¯é»‘è‰²çš„ï¼Ÿ",
            "explanation": "éœ€è¦åˆ¤æ–­'å®ƒ'æŒ‡çš„æ˜¯'çŒ«'è¿˜æ˜¯'å«å­'"
        }
    ]

    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯• {i}:")
        print(f"è¾“å…¥: {case['prompt']}")

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": case['prompt']}],
            temperature=0,
            max_tokens=20
        )

        print(f"è¾“å‡º: {response.choices[0].message.content}")
        print(f"ğŸ’¡ è¿™é‡Œç”¨åˆ°äº†'æ³¨æ„åŠ›': {case['explanation']}")


def demo4_count_tokens():
    """æ¼”ç¤º4ï¼šç†è§£Tokenï¼ˆè¯ï¼‰çš„æ¦‚å¿µ"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º4ï¼šä»€ä¹ˆæ˜¯Tokenï¼ˆè¯ï¼‰ï¼Ÿ")
    print("=" * 60)

    import tiktoken

    # GPTä½¿ç”¨çš„åˆ†è¯å™¨
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

    test_texts = [
        "Hello World",
        "ä½ å¥½ä¸–ç•Œ",
        "äººå·¥æ™ºèƒ½",
        "Artificial Intelligence"
    ]

    for text in test_texts:
        tokens = encoding.encode(text)
        print(f"\næ–‡æœ¬: {text}")
        print(f"Tokenæ•°é‡: {len(tokens)}")
        print(f"Tokenåˆ—è¡¨: {tokens}")
        print(f"è§£ç å›æ¥: {[encoding.decode([t]) for t in tokens]}")

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("   - è‹±æ–‡å•è¯é€šå¸¸æ˜¯1ä¸ªtoken")
    print("   - ä¸­æ–‡å­—ç¬¦é€šå¸¸æ˜¯1-2ä¸ªtoken")
    print("   - Tokenæ˜¯æ¨¡å‹çš„'åŸºæœ¬å•ä½'ï¼Œæ¯æ¬¡ç”Ÿæˆ1ä¸ªtoken")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ“ LLMé›¶åŸºç¡€å®è·µæ•™ç¨‹ ğŸ“".center(60))
    print("\nè¿™ä¸ªæ•™ç¨‹åŒ…å«4ä¸ªæ¼”ç¤ºï¼Œå¸®åŠ©æ‚¨ç†è§£LLMçš„å·¥ä½œåŸç†\n")

    # æ£€æŸ¥API key
    import os
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: è¯·å…ˆè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        print("   export OPENAI_API_KEY='your-key-here'")
        return

    try:
        # æ¼”ç¤º1ï¼šåŸºç¡€ç”Ÿæˆ
        demo1_basic_generation()
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º2ï¼šTemperature
        demo2_temperature_effect()
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º3ï¼šæ³¨æ„åŠ›
        demo3_attention_visualization()
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º4ï¼šToken
        demo4_count_tokens()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ­å–œï¼æ‚¨å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªLLMå®è·µ")
        print("=" * 60)
        print("\næ‚¨ç°åœ¨ç†è§£äº†ï¼š")
        print("âœ… LLMå¦‚ä½•é€è¯ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå›å½’ï¼‰")
        print("âœ… Temperatureå‚æ•°çš„ä½œç”¨")
        print("âœ… ä»€ä¹ˆæ˜¯'æ³¨æ„åŠ›'æœºåˆ¶")
        print("âœ… ä»€ä¹ˆæ˜¯Tokenï¼ˆè¯ï¼‰")
        print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ 02_understand_kv_cache.py ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ä¼˜åŒ–")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\næç¤ºï¼š")
        print("1. ç¡®ä¿å·²å®‰è£…: pip install openai tiktoken")
        print("2. ç¡®ä¿API keyæ­£ç¡®")
        print("3. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")


if __name__ == "__main__":
    main()
