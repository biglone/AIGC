"""
ç¬¬ä¸€ä¸ªLLMå®è·µï¼šä½“éªŒè‡ªå›å½’ç”Ÿæˆï¼ˆOllamaæœ¬åœ°ç‰ˆæœ¬ï¼‰

è¿™ä¸ªè„šæœ¬å±•ç¤ºï¼š
1. LLMå¦‚ä½•ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå›å½’ï¼‰
2. ä¸ºä»€ä¹ˆç”Ÿæˆé•¿æ–‡æœ¬éœ€è¦æ—¶é—´
3. Temperatureå‚æ•°å¦‚ä½•å½±å“è¾“å‡º

ä½¿ç”¨æœ¬åœ°Ollamaï¼Œä¸éœ€è¦API keyï¼

å®‰è£…è¦æ±‚ï¼š
1. å®‰è£…Ollama: curl -fsSL https://ollama.com/install.sh | sh
2. ä¸‹è½½æ¨¡å‹: ollama pull llama3.2:3b
"""

import requests
import json
import sys
import time


def check_ollama():
    """æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œï¼Œå¹¶è¿”å›å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get('models', [])
            if not models:
                print("âŒ Ollamaè¿è¡Œä¸­ï¼Œä½†æ²¡æœ‰å®‰è£…æ¨¡å‹")
                print("\næ¨èå®‰è£…ä»¥ä¸‹æ¨¡å‹ä¹‹ä¸€ï¼š")
                print("  ollama pull qwen2.5:7b        # æœ€æ¨èï¼šæ€§èƒ½å¼ºï¼Œä¸­æ–‡å¥½")
                print("  ollama pull llama3.1:8b       # é€šç”¨æ¨¡å‹")
                print("  ollama pull llama3.2:3b       # è½»é‡å¿«é€Ÿ")
                return None

            print(f"âœ… Ollamaè¿è¡Œä¸­ï¼Œå·²å®‰è£… {len(models)} ä¸ªæ¨¡å‹")
            return models
    except:
        print("âŒ Ollamaæœªè¿è¡Œ")
        print("è¯·å…ˆå¯åŠ¨OllamaæœåŠ¡")
        print("å®‰è£…: curl -fsSL https://ollama.com/install.sh | sh")
        return None


def select_model(models):
    """è®©ç”¨æˆ·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"""
    print(f"\n{'='*60}")
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    print(f"{'='*60}")

    for i, model in enumerate(models, 1):
        name = model['name']
        size_gb = model.get('size', 0) / (1024**3)
        modified = model.get('modified_at', '')[:10]
        print(f"  {i}. {name:35s} ({size_gb:5.1f} GB) - {modified}")

    print(f"\næ¨èæ¨¡å‹ï¼š")
    print(f"  - qwen2.5:7b/14b  : ä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæ€§èƒ½å¥½")
    print(f"  - llama3.1:8b     : é€šç”¨æ€§å¥½")
    print(f"  - deepseek-coder  : ç¼–ç¨‹ä¸“ç”¨")

    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ç¼–å· (1-{len(models)}): ").strip()
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                selected = models[idx]['name']
                print(f"\nâœ… å·²é€‰æ‹©: {selected}")
                return selected
            else:
                print(f"è¯·è¾“å…¥ 1-{len(models)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nå·²å–æ¶ˆ")
            sys.exit(0)


def ollama_generate(prompt, model="llama3.2:3b", temperature=0.7, stream=True, max_tokens=50):
    """è°ƒç”¨Ollama APIç”Ÿæˆæ–‡æœ¬"""
    url = "http://localhost:11434/api/generate"

    data = {
        "model": model,
        "prompt": prompt,
        "stream": stream,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }

    response = requests.post(url, json=data, stream=stream)

    if stream:
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if 'response' in chunk:
                    yield chunk['response']
    else:
        result = response.json()
        return result.get('response', '')


def demo1_basic_generation(model):
    """æ¼”ç¤º1ï¼šåŸºç¡€æ–‡æœ¬ç”Ÿæˆ"""
    print("=" * 60)
    print("æ¼”ç¤º1ï¼šåŸºç¡€æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 60)

    prompt = "äººå·¥æ™ºèƒ½çš„æœªæ¥"
    print(f"\nè¾“å…¥: {prompt}")
    print(f"è¾“å‡º: ", end="", flush=True)

    # æµå¼è¾“å‡ºï¼Œçœ‹åˆ°é€ä¸ªè¯çš„ç”Ÿæˆ
    for token in ollama_generate(prompt, model=model, stream=True, max_tokens=50):
        print(token, end="", flush=True)

    print("\n\nğŸ’¡ è§‚å¯Ÿï¼šæ‚¨çœ‹åˆ°äº†å—ï¼Ÿæ–‡å­—æ˜¯ä¸€ä¸ªä¸€ä¸ªè¹¦å‡ºæ¥çš„ï¼")
    print("   è¿™å°±æ˜¯'è‡ªå›å½’ç”Ÿæˆ' - LLMæ¯æ¬¡åªç”Ÿæˆä¸€ä¸ªè¯")


def demo2_temperature_effect(model):
    """æ¼”ç¤º2ï¼šTemperatureå‚æ•°çš„å½±å“"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º2ï¼šTemperatureå‚æ•°å¦‚ä½•å½±å“è¾“å‡º")
    print("=" * 60)

    prompt = "ä»Šå¤©å¤©æ°”çœŸ"

    print(f"\nè¾“å…¥: {prompt}")

    # æµ‹è¯•ä¸åŒçš„temperature
    temperatures = [0.0, 0.5, 1.0, 1.5]

    for temp in temperatures:
        print(f"\n--- Temperature = {temp} ---")
        print("è¾“å‡º: ", end="", flush=True)

        for token in ollama_generate(prompt, model=model, temperature=temp, stream=True, max_tokens=20):
            print(token, end="", flush=True)
        print()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("   - Temperature = 0.0: æ¯æ¬¡è¾“å‡ºéƒ½ä¸€æ ·ï¼ˆç¡®å®šæ€§ï¼‰")
    print("   - Temperature = 0.5: æ¯”è¾ƒä¿å®ˆï¼Œåˆç†")
    print("   - Temperature = 1.0: æ ‡å‡†ï¼Œæœ‰ä¸€å®šåˆ›é€ æ€§")
    print("   - Temperature = 1.5: å¾ˆéšæœºï¼Œå¯èƒ½ä¸å¤ªåˆç†")


def demo3_attention_visualization(model):
    """æ¼”ç¤º3ï¼šç†è§£'æ³¨æ„åŠ›' - é€šè¿‡ä»»åŠ¡å±•ç¤º"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º3ï¼šç†è§£'æ³¨æ„åŠ›'æœºåˆ¶")
    print("=" * 60)

    # æµ‹è¯•ç”¨ä¾‹ï¼šéœ€è¦"å›å¤´çœ‹"æ‰èƒ½æ­£ç¡®å›ç­”
    test_cases = [
        {
            "prompt": "å°æ˜ä»Šå¤©å»è¶…å¸‚ä¹°äº†è‹¹æœã€‚ä»–å¾ˆå–œæ¬¢åƒæ°´æœã€‚é—®ï¼šè°å–œæ¬¢åƒæ°´æœï¼Ÿè¯·ç®€çŸ­å›ç­”ã€‚",
            "explanation": "æ¨¡å‹éœ€è¦æ³¨æ„åˆ°'ä»–'æŒ‡çš„æ˜¯'å°æ˜'"
        },
        {
            "prompt": "çŒ«ååœ¨å«å­ä¸Šã€‚å®ƒæ˜¯é»‘è‰²çš„ã€‚é—®ï¼šä»€ä¹ˆæ˜¯é»‘è‰²çš„ï¼Ÿè¯·ç®€çŸ­å›ç­”ã€‚",
            "explanation": "éœ€è¦åˆ¤æ–­'å®ƒ'æŒ‡çš„æ˜¯'çŒ«'è¿˜æ˜¯'å«å­'"
        }
    ]

    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯• {i}:")
        print(f"è¾“å…¥: {case['prompt']}")
        print(f"è¾“å‡º: ", end="", flush=True)

        for token in ollama_generate(case['prompt'], model=model, temperature=0, stream=True, max_tokens=30):
            print(token, end="", flush=True)

        print(f"\nğŸ’¡ è¿™é‡Œç”¨åˆ°äº†'æ³¨æ„åŠ›': {case['explanation']}")


def demo4_generation_speed(model):
    """æ¼”ç¤º4ï¼šè§‚å¯Ÿç”Ÿæˆé€Ÿåº¦"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º4ï¼šè§‚å¯Ÿç”Ÿæˆé€Ÿåº¦ï¼ˆæ¯ä¸ªtokençš„æ—¶é—´ï¼‰")
    print("=" * 60)

    prompt = "è¯·å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„çŸ­è¯—"
    print(f"\nè¾“å…¥: {prompt}")
    print(f"\nå¼€å§‹ç”Ÿæˆ...\n")

    token_times = []
    last_time = time.time()
    token_count = 0

    print("è¾“å‡º: ", end="", flush=True)
    for token in ollama_generate(prompt, model=model, stream=True, max_tokens=100):
        current_time = time.time()
        token_times.append(current_time - last_time)
        last_time = current_time
        token_count += 1

        print(token, end="", flush=True)

    avg_time = sum(token_times) / len(token_times) if token_times else 0
    tokens_per_sec = 1 / avg_time if avg_time > 0 else 0

    print(f"\n\nğŸ“Š æ€§èƒ½ç»Ÿè®¡ï¼š")
    print(f"   æ¨¡å‹: {model}")
    print(f"   ç”Ÿæˆtokenæ•°: {token_count}")
    print(f"   å¹³å‡æ¯token: {avg_time*1000:.1f} ms")
    print(f"   ç”Ÿæˆé€Ÿåº¦:   {tokens_per_sec:.1f} tokens/ç§’")
    print(f"\nğŸ’¡ è§‚å¯Ÿï¼šè¿™å°±æ˜¯ä¸ºä»€ä¹ˆé•¿æ–‡æœ¬éœ€è¦æ—¶é—´ï¼")
    print(f"   ç”Ÿæˆ1000ä¸ªè¯å¤§çº¦éœ€è¦ {1000/tokens_per_sec:.1f} ç§’")


def demo5_model_comparison():
    """æ¼”ç¤º5ï¼šå¯¹æ¯”ä¸åŒæ¨¡å‹ï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼‰"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º5ï¼šJetson Thoræ€§èƒ½å±•ç¤º")
    print("=" * 60)

    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    try:
        response = requests.get("http://localhost:11434/api/tags")
        models = response.json().get('models', [])

        if len(models) > 1:
            print(f"\næ‚¨å®‰è£…äº† {len(models)} ä¸ªæ¨¡å‹ï¼Œå¯¹æ¯”ä¸€ä¸‹ï¼š")
            prompt = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿç”¨ä¸€å¥è¯å›ç­”ã€‚"

            for model_info in models[:2]:  # åªæµ‹è¯•å‰2ä¸ª
                model_name = model_info['name']
                print(f"\n--- æ¨¡å‹: {model_name} ---")
                print("è¾“å‡º: ", end="", flush=True)

                start = time.time()
                for token in ollama_generate(prompt, model=model_name, stream=True, max_tokens=50):
                    print(token, end="", flush=True)
                elapsed = time.time() - start

                print(f"\nç”Ÿæˆæ—¶é—´: {elapsed:.2f}ç§’")
        else:
            print(f"\nå½“å‰åªæœ‰1ä¸ªæ¨¡å‹: {models[0]['name']}")
            print("æ‚¨å¯ä»¥ä¸‹è½½æ›´å¤šæ¨¡å‹å¯¹æ¯”ï¼š")
            print("  ollama pull qwen2.5:7b")
            print("  ollama pull llama3.1:8b")

    except Exception as e:
        print(f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ“ LLMé›¶åŸºç¡€å®è·µæ•™ç¨‹ï¼ˆOllamaæœ¬åœ°ç‰ˆï¼‰ğŸ“".center(60))
    print("\nè¿™ä¸ªæ•™ç¨‹åŒ…å«5ä¸ªæ¼”ç¤ºï¼Œå¸®åŠ©æ‚¨ç†è§£LLMçš„å·¥ä½œåŸç†\n")
    print("âœ… å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œä¸éœ€è¦API key")
    print("âœ… è¿è¡Œåœ¨æ‚¨çš„Jetson Thorä¸Š")
    print()

    # æ£€æŸ¥Ollamaå¹¶è·å–æ¨¡å‹åˆ—è¡¨
    models = check_ollama()
    if not models:
        return

    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    selected_model = select_model(models)

    try:
        # æ¼”ç¤º1ï¼šåŸºç¡€ç”Ÿæˆ
        demo1_basic_generation(selected_model)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º2ï¼šTemperature
        demo2_temperature_effect(selected_model)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º3ï¼šæ³¨æ„åŠ›
        demo3_attention_visualization(selected_model)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º4ï¼šç”Ÿæˆé€Ÿåº¦
        demo4_generation_speed(selected_model)
        input("\næŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

        # æ¼”ç¤º5ï¼šæ¨¡å‹å¯¹æ¯”
        demo5_model_comparison()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ­å–œï¼æ‚¨å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªLLMå®è·µï¼ˆæœ¬åœ°ç‰ˆï¼‰")
        print("=" * 60)
        print(f"\nä½¿ç”¨æ¨¡å‹: {selected_model}")
        print("\næ‚¨ç°åœ¨ç†è§£äº†ï¼š")
        print("âœ… LLMå¦‚ä½•é€è¯ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå›å½’ï¼‰")
        print("âœ… Temperatureå‚æ•°çš„ä½œç”¨")
        print("âœ… ä»€ä¹ˆæ˜¯'æ³¨æ„åŠ›'æœºåˆ¶")
        print("âœ… æœ¬åœ°LLMçš„æ€§èƒ½è¡¨ç°")
        print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ 02_understand_kv_cache.py ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ä¼˜åŒ–")

    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("\næç¤ºï¼š")
        print("1. ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
        print("2. ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹: ollama pull llama3.2:3b")
        print("3. æ£€æŸ¥ç½‘ç»œå’Œç«¯å£ 11434 æ˜¯å¦å¯ç”¨")


if __name__ == "__main__":
    main()
