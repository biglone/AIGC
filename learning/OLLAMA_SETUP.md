# ðŸš€ åœ¨Jetson Thorä¸Šä½¿ç”¨Ollamaè¿è¡Œæœ¬åœ°LLM

> å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œä¸éœ€è¦API keyï¼Œå……åˆ†åˆ©ç”¨Jetson Thorçš„å¼ºå¤§æ€§èƒ½ï¼

## å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### 1ï¸âƒ£ å®‰è£…Ollama

```bash
# è‡ªåŠ¨å®‰è£…è„šæœ¬ï¼ˆæ”¯æŒARMæž¶æž„ï¼‰
curl -fsSL https://ollama.com/install.sh | sh
```

å®‰è£…å®ŒæˆåŽï¼ŒOllamaä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡ã€‚

**éªŒè¯å®‰è£…ï¼š**
```bash
# æ£€æŸ¥ç‰ˆæœ¬
ollama --version

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434
```

å¦‚æžœçœ‹åˆ°"Ollama is running"ï¼Œè¯´æ˜Žå®‰è£…æˆåŠŸï¼

---

### 2ï¸âƒ£ ä¸‹è½½æ¨¡åž‹

æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©æ¨¡åž‹å¤§å°ï¼š

```bash
# æŽ¨èï¼šå¿«é€Ÿå…¥é—¨ï¼ˆçº¦2GBï¼‰
ollama pull llama3.2:3b

# è¿›é˜¶ï¼šä¸­æ–‡æ›´å¥½ï¼ˆçº¦4.7GBï¼‰
ollama pull qwen2.5:7b

# é«˜çº§ï¼šæ€§èƒ½æ›´å¼ºï¼ˆçº¦4.7GBï¼‰
ollama pull llama3.1:8b
```

**Jetson Thoré…ç½®å»ºè®®ï¼š**
- 16GBå†…å­˜ï¼šå¯è¿è¡Œ7B-8Bæ¨¡åž‹
- 32GBå†…å­˜ï¼šå¯è¿è¡Œ13Bæ¨¡åž‹
- å»ºè®®å…ˆä¸‹è½½3Bæ¨¡åž‹æµ‹è¯•

**æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡åž‹ï¼š**
```bash
ollama list
```

---

### 3ï¸âƒ£ æµ‹è¯•è¿è¡Œ

```bash
# äº¤äº’å¼å¯¹è¯
ollama run llama3.2:3b

# æµ‹è¯•ä¸­æ–‡
ollama run llama3.2:3b "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"

# é€€å‡ºï¼šè¾“å…¥ /bye
```

---

## ðŸŽ“ è¿è¡Œæ•™ç¨‹

çŽ°åœ¨å¯ä»¥è¿è¡Œæœ¬åœ°ç‰ˆæ•™ç¨‹äº†ï¼š

```bash
cd /home/Biglone/workspace/AIGC/learning

# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source venv/bin/activate

# å®‰è£…ä¾èµ–ï¼ˆåªéœ€è¦requestsï¼‰
pip install requests

# è¿è¡ŒOllamaç‰ˆæ•™ç¨‹
python 01_hello_llm_ollama.py
```

---

## ðŸ“Š æ€§èƒ½å¯¹æ¯”

**Jetson Thorè¿è¡Œä¸åŒæ¨¡åž‹çš„æ€§èƒ½å‚è€ƒï¼š**

| æ¨¡åž‹ | å‚æ•°é‡ | å†…å­˜å ç”¨ | ç”Ÿæˆé€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|-----|--------|----------|----------|----------|
| llama3.2:3b | 3B | ~2GB | ~30 tokens/s | å­¦ä¹ ã€å¿«é€Ÿæµ‹è¯• |
| qwen2.5:7b | 7B | ~4.7GB | ~15 tokens/s | ä¸­æ–‡ä»»åŠ¡ |
| llama3.1:8b | 8B | ~4.7GB | ~12 tokens/s | å¤æ‚æŽ¨ç† |
| qwen2.5:14b | 14B | ~8.5GB | ~8 tokens/s | é«˜æ€§èƒ½éœ€æ±‚ |

*å®žé™…é€Ÿåº¦å–å†³äºŽJetson Thorçš„å…·ä½“é…ç½®*

---

## ðŸ’¡ å¸¸ç”¨å‘½ä»¤

```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡åž‹
ollama list

# åˆ é™¤æ¨¡åž‹
ollama rm llama3.2:3b

# æŸ¥çœ‹æ¨¡åž‹ä¿¡æ¯
ollama show llama3.2:3b

# åœæ­¢OllamaæœåŠ¡
sudo systemctl stop ollama

# å¯åŠ¨OllamaæœåŠ¡
sudo systemctl start ollama

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status ollama
```

---

## ðŸ”§ é«˜çº§é…ç½®

### ä¼˜åŒ–GPUä½¿ç”¨

Jetson Thoræœ‰å¼ºå¤§çš„GPUï¼ŒOllamaä¼šè‡ªåŠ¨ä½¿ç”¨ã€‚æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µï¼š

```bash
# å®žæ—¶ç›‘æŽ§GPU
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨jetsonå·¥å…·
jtop
```

### è‡ªå®šä¹‰æ¨¡åž‹å‚æ•°

åˆ›å»ºè‡ªå®šä¹‰é…ç½®æ–‡ä»¶ï¼š

```bash
# åˆ›å»ºModelfile
cat > CustomModel << EOF
FROM llama3.2:3b
PARAMETER temperature 0.8
PARAMETER num_ctx 4096
EOF

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡åž‹
ollama create my-custom-model -f CustomModel

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡åž‹
ollama run my-custom-model
```

---

## ðŸ› å¸¸è§é—®é¢˜

### Q1: Ollamaå¯åŠ¨å¤±è´¥ï¼Ÿ

**æ£€æŸ¥ï¼š**
```bash
# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u ollama -f

# æ‰‹åŠ¨å¯åŠ¨ï¼ˆè°ƒè¯•ï¼‰
ollama serve
```

### Q2: æ¨¡åž‹ä¸‹è½½å¾ˆæ…¢ï¼Ÿ

**è§£å†³ï¼š**
- ä½¿ç”¨å›½å†…é•œåƒï¼ˆå¦‚æžœæœ‰ï¼‰
- æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡åž‹æ–‡ä»¶åŽå¯¼å…¥

### Q3: å†…å­˜ä¸è¶³ï¼Ÿ

**è§£å†³ï¼š**
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡åž‹
ollama pull llama3.2:1b

# æˆ–ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬
ollama pull llama3.2:3b-q4_0  # 4bité‡åŒ–
```

### Q4: ç«¯å£è¢«å ç”¨ï¼Ÿ

**ä¿®æ”¹é»˜è®¤ç«¯å£ï¼š**
```bash
# è®¾ç½®çŽ¯å¢ƒå˜é‡
export OLLAMA_HOST=0.0.0.0:11435

# é‡å¯æœåŠ¡
sudo systemctl restart ollama
```

---

## ðŸŽ¯ å­¦ä¹ è·¯å¾„

å®ŒæˆOllamaè®¾ç½®åŽï¼š

1. **ä»Šå¤©ï¼ˆ30åˆ†é’Ÿï¼‰ï¼š**
   ```bash
   python 01_hello_llm_ollama.py
   ```
   ç†è§£è‡ªå›žå½’ç”Ÿæˆã€æ³¨æ„åŠ›æœºåˆ¶

2. **æ˜Žå¤©ï¼ˆ30åˆ†é’Ÿï¼‰ï¼š**
   ```bash
   python 02_understand_kv_cache.py
   ```
   ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ä¼˜åŒ–

3. **åŽå¤©ï¼ˆ1å°æ—¶ï¼‰ï¼š**
   - é‡è¯» `docs/01_LLM_Fundamentals.md`
   - çŽ°åœ¨æ‚¨èƒ½ç†è§£æ›´å¤šå†…å®¹äº†

---

## ðŸ“š æŽ¨èæ¨¡åž‹

### ä¸­æ–‡ä»»åŠ¡
- **qwen2.5:7b** - é˜¿é‡ŒQwenç³»åˆ—ï¼Œä¸­æ–‡èƒ½åŠ›å¼º
- **glm4:9b** - æ™ºè°±GLM4ï¼Œä¸­è‹±åŒè¯­

### ç¼–ç¨‹ä»»åŠ¡
- **codellama:7b** - Metaçš„ä»£ç ä¸“ç”¨æ¨¡åž‹
- **deepseek-coder:6.7b** - DeepSeekä»£ç æ¨¡åž‹

### é€šç”¨ä»»åŠ¡
- **llama3.1:8b** - Metaæœ€æ–°ï¼Œæ€§èƒ½å‡è¡¡
- **mistral:7b** - Mistral AIï¼Œé€Ÿåº¦å¿«

### ä¸‹è½½å‘½ä»¤
```bash
ollama pull qwen2.5:7b
ollama pull codellama:7b
ollama pull llama3.1:8b
```

---

## ðŸ”— èµ„æºé“¾æŽ¥

- **Ollamaå®˜ç½‘ï¼š** https://ollama.com/
- **æ¨¡åž‹åº“ï¼š** https://ollama.com/library
- **GitHubï¼š** https://github.com/ollama/ollama
- **æ–‡æ¡£ï¼š** https://github.com/ollama/ollama/blob/main/docs/api.md

---

## âœ¨ ä¼˜åŠ¿

**ä½¿ç”¨Ollamaçš„å¥½å¤„ï¼š**
- âœ… å®Œå…¨å…è´¹ï¼Œæ— ä½¿ç”¨é™åˆ¶
- âœ… æ•°æ®éšç§ï¼Œä¸å‘é€åˆ°äº‘ç«¯
- âœ… ä½Žå»¶è¿Ÿï¼Œæœ¬åœ°è¿è¡Œ
- âœ… ç¦»çº¿å¯ç”¨ï¼Œä¸ä¾èµ–ç½‘ç»œ
- âœ… å……åˆ†åˆ©ç”¨Jetson Thorçš„GPUæ€§èƒ½

---

**å‡†å¤‡å¥½äº†å—ï¼ŸçŽ°åœ¨å°±å¼€å§‹ï¼š**

```bash
# ä¸€é”®å®‰è£…å¹¶ä¸‹è½½æ¨¡åž‹
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b

# è¿è¡Œæ•™ç¨‹
python 01_hello_llm_ollama.py
```

ðŸš€ äº«å—æœ¬åœ°LLMçš„å¼ºå¤§èƒ½åŠ›å§ï¼
